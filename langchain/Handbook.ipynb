{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd2c83ab-5858-460c-a9c2-ec36bffaa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"env_vars.env\")) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f221205-c498-406e-9d21-65255172aa60",
   "metadata": {},
   "source": [
    "# Hugging Face Hub LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c992cb-f75d-4b60-8e31-886656ff865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] output_parser=None partial_variables={} template='Question: {question} Answer: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question} Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "print(prompt)\n",
    "\n",
    "# Initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(repo_id='google/flan-t5-xl', model_kwargs={'temperature':1e-10})\n",
    "\n",
    "# Create prompt template > LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1f9fad3-8f54-4f68-b796-e8019495e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the current chairman of the Federal Reserve?\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "question = \"Who is the current chairman of the Federal Reserve?\"\n",
    "print(question)\n",
    "\n",
    "# NOTE: This will likely time out!\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462b959-9df1-49d9-bcb4-9d5416f7d039",
   "metadata": {},
   "source": [
    "# OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "826585c2-cb7e-4e96-867a-e5640b24c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the model\n",
    "davinci = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad2e5f5d-0214-4f58-8adf-b724370e2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The current chairman of the Federal Reserve is Jerome Powell.\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f8b5b-8254-4be0-96f1-1cef23b18a71",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453f710-c041-4d93-a5ce-94e410b86f79",
   "metadata": {},
   "source": [
    "Two kinds of knowledge that can be \"reasoned\" by LLM:\n",
    "- **Parametric knowlege** -- learned during training and stored in model parameters (weights)\n",
    "- **Source knowledge** -- provided to the model at inference time as examples (via input prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11282157-da27-4105-9504-77707050db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The meaning of life is to find your own purpose and make the most of it. Life is what you make it!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: What is the meaning of life? AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0 # increase creativity/randomness of output\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e59ae676-c1a4-40d8-bac2-1efc3dcccea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The meaning of life is to live it to the fullest and enjoy every moment.\n"
     ]
    }
   ],
   "source": [
    "# To help the model, we can give it a few examples of the type of answers we’d like\n",
    "prompt = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: How are you? AI: I can't complain but sometimes I still do. \n",
    "User: What time is it? AI: It's time to get a watch. User: What is the meaning of life? AI: \"\"\"\n",
    "\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371fe9f8-4c85-4e09-a1c3-0cdf1be359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate caters to source knowledge input\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\" User: {query} AI: {answer} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd45bcb-b5df-46c1-8638-175ce7cfed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query', 'answer'] output_parser=None partial_variables={} template=' User: {query} AI: {answer} ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af5bc184-eaa4-4bf4-a473-d15458a1857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}] example_selector=None example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: \"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\" User: {query} AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt,\n",
    "                                                 prefix=prefix, suffix=suffix, input_variables=[\"query\"],\n",
    "                                                 example_separator=\"\\n\\n\")\n",
    "print(few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d17b7db-1546-4543-910b-489f4bcd623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      "\n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      "\n",
      " User: What time is it? AI: It's time to get a watch. \n",
      "\n",
      " User: What is the meaning of life? AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f31ec6c-cb32-48c3-9597-7a256638ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "affb28f2-d05c-484e-88a4-70aa386c128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}]\n"
     ]
    }
   ],
   "source": [
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed46cd6-2795-4550-8d09-14ebbb3c8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}] example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) get_text_length=<function _get_length_based at 0x11b59e3b0> max_length=50 example_text_lengths=[15, 14, 11, 16, 10, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(examples=examples,\n",
    "                                              example_prompt=example_prompt,\n",
    "                                              max_length=50 # this sets the max length that examples should be\n",
    "                                             )\n",
    "print(example_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f47011ae-51b3-4fd3-ae38-e90b2011a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=None example_selector=LengthBasedExampleSelector(examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}], example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True), get_text_length=<function _get_length_based at 0x11b59e3b0>, max_length=50, example_text_lengths=[15, 14, 11, 16, 10, 19, 19]) example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "print(dynamic_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eb4caf4-3121-4861-b612-fad860bd007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      " User: What time is it? AI: It's time to get a watch. \n",
      " User: What is the meaning of life? AI: 42 \n",
      " User: How do birds fly? AI: \n"
     ]
    }
   ],
   "source": [
    "prompt = dynamic_prompt_template.format(query=\"How do birds fly?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcc4db1-a3f9-48b4-a169-07b8f3a4e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With a little help from their friends!\n"
     ]
    }
   ],
   "source": [
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fa83-d1a5-40b5-8866-085819ae3d59",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f6ca07-ac51-4ce8-9c79-914dabbc8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8998c1-e423-4d96-abae-badb0e7fdb99",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c754fb0-e597-474a-950d-3dd06b909325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fbfbf6c-e315-4882-9589-e3ecff44bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10361c19-f7d6-4194-abce-9fc4d094139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5c2b73-dc41-416c-9f6f-ed4a8d69291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a4cab-a335-4114-af59-6fcff7378fe6",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f03f0c1-79ad-4a38-aa7a-5aa76bd6a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_sum = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88f5c3bb-8eb6-40b5-8cef-b256f7771796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(conversation_sum, \"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "211e33bc-a246-412e-a690-686f6668cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 383 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an exciting goal! What kind of steps do you think you need to take to achieve it?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"My interest here is to get filthy rich and live a lavish life!\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f620a2d-dad3-4d2d-ade5-13d0d9332a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 539 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a great goal! I think the first step is to identify what resources you have available to you. Do you have any savings, investments, or other assets that you can use to help you reach your goal?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"I just want to analyze the different possibilities to that goal. What can you think of?\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57f592ea-ee56-4e77-8dcd-011afcdc1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI and the AI responded with a greeting and asked how it could help. The human expressed their goal of getting rich and living a lavish life, to which the AI responded positively and asked what steps the human thought they needed to take to achieve it. The human asked the AI to analyze the different possibilities to that goal, and the AI suggested the first step should be to identify what resources the human had available to them.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7c51e-b86c-4460-a8da-a65180487c38",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a5c72c2-088e-4f7e-8919-65aaec726e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acts in the same way as our earlier “buffer memory” but adds a window to the memory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# We set k=1 -- meaning the window will remember the single latest interaction between the human and AI\n",
    "conversation_bufw = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "437b37c8-ab12-4435-b6b7-74a6abf1b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(inputs=[])['history']\n",
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02631ea7-2246-4eb8-bd3b-70609ae11ee6",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20f9e2d0-2a81-419f-9f0e-8509f9dba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes the earliest interactions in a conversation while maintaining the most recent tokens in the conversation\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation_sum_bufw = ConversationChain(llm=llm, \n",
    "                                          memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e894f6-cd02-46c5-a993-102e2084bdb4",
   "metadata": {},
   "source": [
    "This the only one of our memory types (so far) that allows us to remember distant interactions and store the most recent interactions in their raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ff9da27-a6c2-47ee-9ec3-ec8661a8771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others:\n",
    "# Knowledge graph\n",
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "\n",
    "# Entities -- remember details about the entities (persons, etc.)\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda678c-47de-4827-9ced-599419956777",
   "metadata": {},
   "source": [
    "# Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399515f-6395-49dd-86c0-bdc382797c53",
   "metadata": {},
   "source": [
    "The external knowledge base is the \"window\" into the world beyond the LLM’s training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf560344-e9bf-4c6f-82bf-85bf7956a9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.simple to /Users/trucvietle/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1.66k/1.66k [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 235M/235M [00:22<00:00, 10.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to /Users/trucvietle/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset # from Hugging Face\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2bbd864-041c-45c7-a6ba-1267aea76750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '63',\n",
       " 'url': 'https://simple.wikipedia.org/wiki/Acceleration',\n",
       " 'title': 'Acceleration',\n",
       " 'text': 'Acceleration is a measure of how fast velocity changes. Acceleration is the change of velocity divided by the change of time. Acceleration is a vector, and therefore includes both a size and a direction. Acceleration is also a change in speed and direction, there is:\\n\\nSpeed (a scalar quantity) (uses no direction)\\n\\n Distance is how far you traveled \\n Time is how long it took you to travel \\n Speed is how fast you are moving - Speed = Distance / Time \\n\\nVelocity (a vector quantity) (uses a direction)\\n\\n Displacement is how much your position has changed in what direction\\n Velocity is how quickly your position is changing and in what direction\\n Velocity = Displacement / Time \\n\\nThe measurement of how fast acceleration changes is called jerk.\\n\\nExamples \\n An object was moving north at 10 meters per second. The object speeds up and now is moving north at 17 meters per second. The object has accelerated. \\n An apple is falling down. It starts falling at 0 meters per second. At the end of the first second, the apple is moving at 9.8 meters per second. The apple has accelerated. At the end of the second second, the apple is moving down at 19.6 meters per second. The apple has accelerated again.\\n Jane is walking east at 3 kilometers per hour. Jane\\'s velocity does not change. Jane\\'s acceleration is zero. \\n Tom was walking east at 3 kilometers per hour. Tom turns and walks south at 3 kilometers per hour. Tom has had a nonzero acceleration. \\n Sally was walking east at 3 kilometers per hour. Sally slows down. After, Sally walks east at 1.5 kilometers per hour. Sally has had a nonzero acceleration.\\n Acceleration due to gravity\\n\\nFinding acceleration \\n\\nAcceleration is the rate of change of the velocity of an object. Acceleration  can be found by using:\\n\\nwhere\\n is the velocity at the start\\n s the time at the start\\n is the time at the end\\n\\nSometimes the change in velocity  is written as Δ. Sometimes the change in time  is written as Δt.\\n\\nIn difficult situations, the acceleration can be calculated using mathematics: in calculus, acceleration is the derivative of the velocity (with respect to time), .\\n\\nUnits of measurement \\nAcceleration has its own units of measurement. For example, if velocity is measured in meters per second, and if time is measured in seconds, then acceleration is measured in meters per second squared  (m/s2).\\n\\nOther words \\nAcceleration can be positive or negative. When the acceleration is negative (but the velocity does not change direction), it is sometimes called deceleration. For example, when a car brakes it decelerates. Physicists usually only use the word \"acceleration\".\\n\\nNewton\\'s second law of motion \\nNewton\\'s laws of motion are rules for how things move. These rules are called \"laws of motion\". Isaac Newton is the scientist who first wrote down the main laws of motion.\\nAccording to Newton\\'s Second Law of Motion, the force something needs to accelerate an object depends on the object\\'s mass (the amount of \"stuff\" the object is made from or how \"heavy\" it is).\\nThe formula of Newton\\'s Second Law of Motion is ,\\nwhere  is the acceleration,  is the force, and  the mass. \\nThis formula is very well-known, and it is very important in physics. Newton\\'s Second Law of Motion, in short \"Newton\\'s Second Law\", is often one of the first things that physics students learn.\\n\\nDeceleration \\nDeceleration is negative or backwards acceleration. This means that something slows down instead of speeding up. For example, when a car brakes, it is decelerating.\\n\\nBasic physics ideas\\nMechanics'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc573c43-c18a-4584-b6fa-96ad305a6b92",
   "metadata": {},
   "source": [
    "## Create chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db573-43fa-49a5-8c6f-4fbc520fc8ba",
   "metadata": {},
   "source": [
    "A token is typically the size of a word or sub-word and varies by LLM. The tokens themselves are built using a *tokenizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c3a7438-b377-4477-aa39-1d31441075d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    \n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    \n",
    "    return len(tokens)\n",
    "\n",
    "text = \"hello I am a chunk of text and using the tiktoken_len function \\\n",
    "we can find the length of this chunk of text in tokens\"\n",
    "\n",
    "print(tiktoken_len(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03d47e65-2a01-4711-a573-f4be9c3455d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Acceleration is a measure of how fast velocity changes. Acceleration is the change of velocity divided by the change of time. Acceleration is a vector, and therefore includes both a size and a direction. Acceleration is also a change in speed and direction, there is:\\n\\nSpeed (a scalar quantity) (uses no direction)\\n\\n Distance is how far you traveled \\n Time is how long it took you to travel \\n Speed is how fast you are moving - Speed = Distance / Time \\n\\nVelocity (a vector quantity) (uses a direction)\\n\\n Displacement is how much your position has changed in what direction\\n Velocity is how quickly your position is changing and in what direction\\n Velocity = Displacement / Time \\n\\nThe measurement of how fast acceleration changes is called jerk.\\n\\nExamples \\n An object was moving north at 10 meters per second. The object speeds up and now is moving north at 17 meters per second. The object has accelerated. \\n An apple is falling down. It starts falling at 0 meters per second. At the end of the first second, the apple is moving at 9.8 meters per second. The apple has accelerated. At the end of the second second, the apple is moving down at 19.6 meters per second. The apple has accelerated again.\\n Jane is walking east at 3 kilometers per hour. Jane's velocity does not change. Jane's acceleration is zero. \\n Tom was walking east at 3 kilometers per hour. Tom turns and walks south at 3 kilometers per hour. Tom has had a nonzero acceleration. \\n Sally was walking east at 3 kilometers per hour. Sally slows down. After, Sally walks east at 1.5 kilometers per hour. Sally has had a nonzero acceleration.\\n Acceleration due to gravity\\n\\nFinding acceleration\",\n",
       " 'Finding acceleration \\n\\nAcceleration is the rate of change of the velocity of an object. Acceleration  can be found by using:\\n\\nwhere\\n is the velocity at the start\\n s the time at the start\\n is the time at the end\\n\\nSometimes the change in velocity  is written as Δ. Sometimes the change in time  is written as Δt.\\n\\nIn difficult situations, the acceleration can be calculated using mathematics: in calculus, acceleration is the derivative of the velocity (with respect to time), .\\n\\nUnits of measurement \\nAcceleration has its own units of measurement. For example, if velocity is measured in meters per second, and if time is measured in seconds, then acceleration is measured in meters per second squared  (m/s2).\\n\\nOther words \\nAcceleration can be positive or negative. When the acceleration is negative (but the velocity does not change direction), it is sometimes called deceleration. For example, when a car brakes it decelerates. Physicists usually only use the word \"acceleration\".\\n\\nNewton\\'s second law of motion \\nNewton\\'s laws of motion are rules for how things move. These rules are called \"laws of motion\". Isaac Newton is the scientist who first wrote down the main laws of motion.\\nAccording to Newton\\'s Second Law of Motion, the force something needs to accelerate an object depends on the object\\'s mass (the amount of \"stuff\" the object is made from or how \"heavy\" it is).\\nThe formula of Newton\\'s Second Law of Motion is ,\\nwhere  is the acceleration,  is the force, and  the mass. \\nThis formula is very well-known, and it is very important in physics. Newton\\'s Second Law of Motion, in short \"Newton\\'s Second Law\", is often one of the first things that physics students learn.',\n",
       " 'Deceleration \\nDeceleration is negative or backwards acceleration. This means that something slows down instead of speeding up. For example, when a car brakes, it is decelerating.\\n\\nBasic physics ideas\\nMechanics']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20,\n",
    "                                               length_function=tiktoken_len,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "chunks = text_splitter.split_text(data[36]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e1ac258-074d-4f16-b3a1-77f757a7b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(374, 392, 48)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chunks))\n",
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d38a37-e2db-418b-a3f4-7627cbf1f924",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78b020f0-e2a7-422a-8735-88200be90d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# embed = OpenAIEmbeddings()\n",
    "embed = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23e153b2-05ad-4efb-b8d5-3dd42f0f6f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['this is the first chunk of text',\n",
    "         'then another second chunk of text is here']\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094457f3-f5f8-44cc-9418-5a93a5e3bcd1",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "572cb5d1-c177-4adf-b661-9e15a902bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = '../docs/chroma/'\n",
    "\n",
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b0b8f8b-94f8-481b-85d2-ffd5ceef6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:08<00:00, 1234.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_limit = 100\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'wiki-id': str(record['id']),\n",
    "        'source': record['url'],\n",
    "        'title': record['title']\n",
    "    }\n",
    "    \n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "                                                 \n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata} for j, text in enumerate(record_texts)]\n",
    "    \n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    \n",
    "#     # if we have reached the batch_limit we can add texts\n",
    "#     if len(texts) >= batch_limit:\n",
    "#         embeds = embed.embed_documents(texts)\n",
    "        \n",
    "#         vectordb = Chroma.from_documents(\n",
    "#             documents=texts,\n",
    "#             embedding=embeds,\n",
    "#             persist_directory=persist_directory\n",
    "#         )\n",
    "        \n",
    "#         texts = []\n",
    "#         metadatas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f5f9e-69ca-4912-92cd-01e01514352a",
   "metadata": {},
   "source": [
    "## Query vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf4da79f-0409-4203-b669-6dc7d9419a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed)\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6839f63-d6db-47ec-a1bf-41bb661da72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(i) “Electric Vehicle Charging” is defined as Card Transactions classified under the MCC 5552  \\n(Electric Vehicle Charging).  \\n \\nB. 0.25% cashback on the following transactions:  \\n• all Card Transactions if the Minimum Spend Requirement is not met,  \\n• all Card Transaction s (excluding Card Transactions under the categories listed at Clause \\n2A) if the Minimum Spend Requirement is met; and  \\n• Card Transactions expressly stated in Clause 2A(a)(iii) above.  \\n  \\n3. Minimum Spend Requirement  \\n \\na) The Minimum Spend Requirement must be met to receive the Cashback listed at Clause 2A \\nabove. There is no minimum spend requirement to receive the Base Cashback. The Minimum \\nSpend Requirement can be aggregated between the Principal and Supplementary Cardmembers. \\nThe maximum amount of Cashback t hat one account (the Principal and all Supplementary \\nCardmembers together) can earn in any calendar month is S$160.  \\n \\nMinimum Spend \\nRequirement  Cashback Cap', metadata={'source': '../docs/tncs-365cc-programme.pdf', 'page': 2}),\n",
       " Document(page_content='Account by the Principal and Supplementary Cardmembers in each calendar month , but \\nexcludes the Exclusio ns.  \\n \\nc) “Cashback” refers to the cashback awarded every calendar month under Clause 2 below.  \\n \\nd) “Exclusions” refers to the Card Transactions described under Clause 3(b) below.  \\n \\ne) “Minimum Spend Requirement” refers to all minimum spend requirements set out under Clause \\n3(a) below.  \\n \\n2. Cashback  \\n \\nThe OCBC 365 Card shall come with the following features:  \\n \\nA. Subject to the Minimum Spend Requirement  being met:  \\n \\na) 5% cashback on “Dining” Card Transactions  \\n \\n(i) “Dining” is defined as Card Transactions made in at all restaurants & cafes, caterers and \\nfast food restaurants classified under the Merchant Category Code  (“MCC”):  \\n• MCC 5812 (Restaurants and eating  places);  \\n• MCC 5814 (Fast Food Restaurants);  and \\n• MCC 5811  (Caterers).  \\n \\n(ii)  Cashback on Dining includes  :', metadata={'source': '../docs/tncs-365cc-programme.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the minimum spend?\"\n",
    "\n",
    "vectorstore.similarity_search(query, # our search query\n",
    "                              k=3 # return 3 most relevant docs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc2124-fcda-4e1e-ae32-8e347ff2fa8a",
   "metadata": {},
   "source": [
    "## Generative QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "db8893e0-97d6-4739-8d10-ecc719e34e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The minimum spend requirement is the amount of money that needs to be spent on the OCBC 365 Card in order to qualify for certain cashback rewards. The specific amount of the minimum spend requirement is not mentioned in the given context.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a330ea26-68b7-449b-b262-21668bcaea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Are there any dining rewards?',\n",
       " 'answer': 'Yes, there are dining rewards. The OCBC 365 Card offers 5% cashback on \"Dining\" card transactions, which includes transactions made at restaurants, cafes, caterers, and fast food restaurants. However, transactions made at hotels, wedding banquets, and transactions not made in person (such as telephone or mail orders) are not eligible for the 5% cashback. \\n',\n",
       " 'sources': '../docs/tncs-365cc-programme.pdf'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
    "                                                              retriever=vectorstore.as_retriever()\n",
    "                                                             )\n",
    "\n",
    "query = \"Are there any dining rewards?\"\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba896-df2b-4bfe-8977-83be0f0cd02c",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a4e5-0c6f-4155-ad07-18cbc4e2fb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
