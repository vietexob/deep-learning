{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2c83ab-5858-460c-a9c2-ec36bffaa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"env_vars.env\")) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f221205-c498-406e-9d21-65255172aa60",
   "metadata": {},
   "source": [
    "# Hugging Face Hub LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c992cb-f75d-4b60-8e31-886656ff865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] output_parser=None partial_variables={} template='Question: {question} Answer: ' template_format='f-string' validate_template=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trucvietle/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question} Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "print(prompt)\n",
    "\n",
    "# Initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(repo_id='google/flan-t5-xl', model_kwargs={'temperature':1e-10})\n",
    "\n",
    "# Create prompt template > LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f9fad3-8f54-4f68-b796-e8019495e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the richest person in the world?\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "question = \"Who is the richest person in the world?\"\n",
    "print(question)\n",
    "\n",
    "# NOTE: This will likely time out!\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462b959-9df1-49d9-bcb4-9d5416f7d039",
   "metadata": {},
   "source": [
    "# OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826585c2-cb7e-4e96-867a-e5640b24c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the model\n",
    "davinci = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2e5f5d-0214-4f58-8adf-b724370e2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The richest person in the world as of October 2020 is Amazon founder Jeff Bezos, with a net worth of approximately $182 billion.\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f8b5b-8254-4be0-96f1-1cef23b18a71",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453f710-c041-4d93-a5ce-94e410b86f79",
   "metadata": {},
   "source": [
    "Two kinds of knowledge that can be \"reasoned\" by LLM:\n",
    "- **Parametric knowlege** -- learned during training and stored in model parameters (weights)\n",
    "- **Source knowledge** -- provided to the model at inference time as contexts and examples (via input prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11282157-da27-4105-9504-77707050db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"That's easy. All you have to do is find someone who already knows how to solve it and then copy them!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: How to solve a Rubik's Cube? AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0 # increase creativity/randomness of output\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59ae676-c1a4-40d8-bac2-1efc3dcccea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Well, that's a bit out of my area of expertise - I'm more of a conversational AI. But I'm sure you'll figure it out!\n"
     ]
    }
   ],
   "source": [
    "# To help the model, we can give it a few examples of the type of answers we’d like\n",
    "prompt = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: How are you? AI: I can't complain but sometimes I still do. \n",
    "User: What time is it? AI: It's time to get a watch. User: How to solve a matrix rotation problem? AI: \"\"\"\n",
    "\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371fe9f8-4c85-4e09-a1c3-0cdf1be359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate caters to source knowledge input\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\" User: {query} AI: {answer} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd45bcb-b5df-46c1-8638-175ce7cfed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query', 'answer'] output_parser=None partial_variables={} template=' User: {query} AI: {answer} ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af5bc184-eaa4-4bf4-a473-d15458a1857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}] example_selector=None example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions (contexts)\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: \"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\" User: {query} AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt,\n",
    "                                                 prefix=prefix, suffix=suffix, input_variables=[\"query\"],\n",
    "                                                 example_separator=\"\\n\\n\")\n",
    "print(few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d17b7db-1546-4543-910b-489f4bcd623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      "\n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      "\n",
      " User: What time is it? AI: It's time to get a watch. \n",
      "\n",
      " User: What is the meaning of life? AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f31ec6c-cb32-48c3-9597-7a256638ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "affb28f2-d05c-484e-88a4-70aa386c128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}]\n"
     ]
    }
   ],
   "source": [
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed46cd6-2795-4550-8d09-14ebbb3c8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}] example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) get_text_length=<function _get_length_based at 0x11b59e3b0> max_length=50 example_text_lengths=[15, 14, 11, 16, 10, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(examples=examples,\n",
    "                                              example_prompt=example_prompt,\n",
    "                                              max_length=50 # this sets the max length that examples should be\n",
    "                                             )\n",
    "print(example_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f47011ae-51b3-4fd3-ae38-e90b2011a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=None example_selector=LengthBasedExampleSelector(examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}], example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True), get_text_length=<function _get_length_based at 0x11b59e3b0>, max_length=50, example_text_lengths=[15, 14, 11, 16, 10, 19, 19]) example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "print(dynamic_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eb4caf4-3121-4861-b612-fad860bd007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      " User: What time is it? AI: It's time to get a watch. \n",
      " User: What is the meaning of life? AI: 42 \n",
      " User: How do birds fly? AI: \n"
     ]
    }
   ],
   "source": [
    "prompt = dynamic_prompt_template.format(query=\"How do birds fly?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcc4db1-a3f9-48b4-a169-07b8f3a4e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With a little help from their friends!\n"
     ]
    }
   ],
   "source": [
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fa83-d1a5-40b5-8866-085819ae3d59",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f6ca07-ac51-4ce8-9c79-914dabbc8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8998c1-e423-4d96-abae-badb0e7fdb99",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c754fb0-e597-474a-950d-3dd06b909325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fbfbf6c-e315-4882-9589-e3ecff44bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10361c19-f7d6-4194-abce-9fc4d094139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5c2b73-dc41-416c-9f6f-ed4a8d69291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a4cab-a335-4114-af59-6fcff7378fe6",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f03f0c1-79ad-4a38-aa7a-5aa76bd6a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_sum = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88f5c3bb-8eb6-40b5-8cef-b256f7771796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(conversation_sum, \"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "211e33bc-a246-412e-a690-686f6668cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 383 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an exciting goal! What kind of steps do you think you need to take to achieve it?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"My interest here is to get filthy rich and live a lavish life!\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f620a2d-dad3-4d2d-ade5-13d0d9332a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 539 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a great goal! I think the first step is to identify what resources you have available to you. Do you have any savings, investments, or other assets that you can use to help you reach your goal?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"I just want to analyze the different possibilities to that goal. What can you think of?\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57f592ea-ee56-4e77-8dcd-011afcdc1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI and the AI responded with a greeting and asked how it could help. The human expressed their goal of getting rich and living a lavish life, to which the AI responded positively and asked what steps the human thought they needed to take to achieve it. The human asked the AI to analyze the different possibilities to that goal, and the AI suggested the first step should be to identify what resources the human had available to them.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7c51e-b86c-4460-a8da-a65180487c38",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a5c72c2-088e-4f7e-8919-65aaec726e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acts in the same way as our earlier “buffer memory” but adds a window to the memory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# We set k=1 -- meaning the window will remember the single latest interaction between the human and AI\n",
    "conversation_bufw = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "437b37c8-ab12-4435-b6b7-74a6abf1b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(inputs=[])['history']\n",
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02631ea7-2246-4eb8-bd3b-70609ae11ee6",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20f9e2d0-2a81-419f-9f0e-8509f9dba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes the earliest interactions in a conversation while maintaining the most recent tokens in the conversation\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation_sum_bufw = ConversationChain(llm=llm, \n",
    "                                          memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e894f6-cd02-46c5-a993-102e2084bdb4",
   "metadata": {},
   "source": [
    "This the only one of our memory types (so far) that allows us to remember distant interactions and store the most recent interactions in their raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ff9da27-a6c2-47ee-9ec3-ec8661a8771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others:\n",
    "# Knowledge graph\n",
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "\n",
    "# Entities -- remember details about the entities (persons, etc.)\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda678c-47de-4827-9ced-599419956777",
   "metadata": {},
   "source": [
    "# Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399515f-6395-49dd-86c0-bdc382797c53",
   "metadata": {},
   "source": [
    "The external knowledge base is the \"window\" into the world beyond the LLM’s training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf560344-e9bf-4c6f-82bf-85bf7956a9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.simple to /Users/trucvietle/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1.66k/1.66k [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 235M/235M [00:22<00:00, 10.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to /Users/trucvietle/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset # from Hugging Face\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2bbd864-041c-45c7-a6ba-1267aea76750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '63',\n",
       " 'url': 'https://simple.wikipedia.org/wiki/Acceleration',\n",
       " 'title': 'Acceleration',\n",
       " 'text': 'Acceleration is a measure of how fast velocity changes. Acceleration is the change of velocity divided by the change of time. Acceleration is a vector, and therefore includes both a size and a direction. Acceleration is also a change in speed and direction, there is:\\n\\nSpeed (a scalar quantity) (uses no direction)\\n\\n Distance is how far you traveled \\n Time is how long it took you to travel \\n Speed is how fast you are moving - Speed = Distance / Time \\n\\nVelocity (a vector quantity) (uses a direction)\\n\\n Displacement is how much your position has changed in what direction\\n Velocity is how quickly your position is changing and in what direction\\n Velocity = Displacement / Time \\n\\nThe measurement of how fast acceleration changes is called jerk.\\n\\nExamples \\n An object was moving north at 10 meters per second. The object speeds up and now is moving north at 17 meters per second. The object has accelerated. \\n An apple is falling down. It starts falling at 0 meters per second. At the end of the first second, the apple is moving at 9.8 meters per second. The apple has accelerated. At the end of the second second, the apple is moving down at 19.6 meters per second. The apple has accelerated again.\\n Jane is walking east at 3 kilometers per hour. Jane\\'s velocity does not change. Jane\\'s acceleration is zero. \\n Tom was walking east at 3 kilometers per hour. Tom turns and walks south at 3 kilometers per hour. Tom has had a nonzero acceleration. \\n Sally was walking east at 3 kilometers per hour. Sally slows down. After, Sally walks east at 1.5 kilometers per hour. Sally has had a nonzero acceleration.\\n Acceleration due to gravity\\n\\nFinding acceleration \\n\\nAcceleration is the rate of change of the velocity of an object. Acceleration  can be found by using:\\n\\nwhere\\n is the velocity at the start\\n s the time at the start\\n is the time at the end\\n\\nSometimes the change in velocity  is written as Δ. Sometimes the change in time  is written as Δt.\\n\\nIn difficult situations, the acceleration can be calculated using mathematics: in calculus, acceleration is the derivative of the velocity (with respect to time), .\\n\\nUnits of measurement \\nAcceleration has its own units of measurement. For example, if velocity is measured in meters per second, and if time is measured in seconds, then acceleration is measured in meters per second squared  (m/s2).\\n\\nOther words \\nAcceleration can be positive or negative. When the acceleration is negative (but the velocity does not change direction), it is sometimes called deceleration. For example, when a car brakes it decelerates. Physicists usually only use the word \"acceleration\".\\n\\nNewton\\'s second law of motion \\nNewton\\'s laws of motion are rules for how things move. These rules are called \"laws of motion\". Isaac Newton is the scientist who first wrote down the main laws of motion.\\nAccording to Newton\\'s Second Law of Motion, the force something needs to accelerate an object depends on the object\\'s mass (the amount of \"stuff\" the object is made from or how \"heavy\" it is).\\nThe formula of Newton\\'s Second Law of Motion is ,\\nwhere  is the acceleration,  is the force, and  the mass. \\nThis formula is very well-known, and it is very important in physics. Newton\\'s Second Law of Motion, in short \"Newton\\'s Second Law\", is often one of the first things that physics students learn.\\n\\nDeceleration \\nDeceleration is negative or backwards acceleration. This means that something slows down instead of speeding up. For example, when a car brakes, it is decelerating.\\n\\nBasic physics ideas\\nMechanics'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc573c43-c18a-4584-b6fa-96ad305a6b92",
   "metadata": {},
   "source": [
    "## Create chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db573-43fa-49a5-8c6f-4fbc520fc8ba",
   "metadata": {},
   "source": [
    "A token is typically the size of a word or sub-word and varies by LLM. The tokens themselves are built using a *tokenizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c3a7438-b377-4477-aa39-1d31441075d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    \n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    \n",
    "    return len(tokens)\n",
    "\n",
    "text = \"hello I am a chunk of text and using the tiktoken_len function \\\n",
    "we can find the length of this chunk of text in tokens\"\n",
    "\n",
    "print(tiktoken_len(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03d47e65-2a01-4711-a573-f4be9c3455d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Acceleration is a measure of how fast velocity changes. Acceleration is the change of velocity divided by the change of time. Acceleration is a vector, and therefore includes both a size and a direction. Acceleration is also a change in speed and direction, there is:\\n\\nSpeed (a scalar quantity) (uses no direction)\\n\\n Distance is how far you traveled \\n Time is how long it took you to travel \\n Speed is how fast you are moving - Speed = Distance / Time \\n\\nVelocity (a vector quantity) (uses a direction)\\n\\n Displacement is how much your position has changed in what direction\\n Velocity is how quickly your position is changing and in what direction\\n Velocity = Displacement / Time \\n\\nThe measurement of how fast acceleration changes is called jerk.\\n\\nExamples \\n An object was moving north at 10 meters per second. The object speeds up and now is moving north at 17 meters per second. The object has accelerated. \\n An apple is falling down. It starts falling at 0 meters per second. At the end of the first second, the apple is moving at 9.8 meters per second. The apple has accelerated. At the end of the second second, the apple is moving down at 19.6 meters per second. The apple has accelerated again.\\n Jane is walking east at 3 kilometers per hour. Jane's velocity does not change. Jane's acceleration is zero. \\n Tom was walking east at 3 kilometers per hour. Tom turns and walks south at 3 kilometers per hour. Tom has had a nonzero acceleration. \\n Sally was walking east at 3 kilometers per hour. Sally slows down. After, Sally walks east at 1.5 kilometers per hour. Sally has had a nonzero acceleration.\\n Acceleration due to gravity\\n\\nFinding acceleration\",\n",
       " 'Finding acceleration \\n\\nAcceleration is the rate of change of the velocity of an object. Acceleration  can be found by using:\\n\\nwhere\\n is the velocity at the start\\n s the time at the start\\n is the time at the end\\n\\nSometimes the change in velocity  is written as Δ. Sometimes the change in time  is written as Δt.\\n\\nIn difficult situations, the acceleration can be calculated using mathematics: in calculus, acceleration is the derivative of the velocity (with respect to time), .\\n\\nUnits of measurement \\nAcceleration has its own units of measurement. For example, if velocity is measured in meters per second, and if time is measured in seconds, then acceleration is measured in meters per second squared  (m/s2).\\n\\nOther words \\nAcceleration can be positive or negative. When the acceleration is negative (but the velocity does not change direction), it is sometimes called deceleration. For example, when a car brakes it decelerates. Physicists usually only use the word \"acceleration\".\\n\\nNewton\\'s second law of motion \\nNewton\\'s laws of motion are rules for how things move. These rules are called \"laws of motion\". Isaac Newton is the scientist who first wrote down the main laws of motion.\\nAccording to Newton\\'s Second Law of Motion, the force something needs to accelerate an object depends on the object\\'s mass (the amount of \"stuff\" the object is made from or how \"heavy\" it is).\\nThe formula of Newton\\'s Second Law of Motion is ,\\nwhere  is the acceleration,  is the force, and  the mass. \\nThis formula is very well-known, and it is very important in physics. Newton\\'s Second Law of Motion, in short \"Newton\\'s Second Law\", is often one of the first things that physics students learn.',\n",
       " 'Deceleration \\nDeceleration is negative or backwards acceleration. This means that something slows down instead of speeding up. For example, when a car brakes, it is decelerating.\\n\\nBasic physics ideas\\nMechanics']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20,\n",
    "                                               length_function=tiktoken_len,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "chunks = text_splitter.split_text(data[36]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e1ac258-074d-4f16-b3a1-77f757a7b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(374, 392, 48)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chunks))\n",
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d38a37-e2db-418b-a3f4-7627cbf1f924",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78b020f0-e2a7-422a-8735-88200be90d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# embed = OpenAIEmbeddings()\n",
    "embed = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23e153b2-05ad-4efb-b8d5-3dd42f0f6f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['this is the first chunk of text',\n",
    "         'then another second chunk of text is here']\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094457f3-f5f8-44cc-9418-5a93a5e3bcd1",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "572cb5d1-c177-4adf-b661-9e15a902bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = '../docs/chroma/'\n",
    "\n",
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b0b8f8b-94f8-481b-85d2-ffd5ceef6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:08<00:00, 1234.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_limit = 100\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'wiki-id': str(record['id']),\n",
    "        'source': record['url'],\n",
    "        'title': record['title']\n",
    "    }\n",
    "    \n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "                                                 \n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata} for j, text in enumerate(record_texts)]\n",
    "    \n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    \n",
    "#     # if we have reached the batch_limit we can add texts\n",
    "#     if len(texts) >= batch_limit:\n",
    "#         embeds = embed.embed_documents(texts)\n",
    "        \n",
    "#         vectordb = Chroma.from_documents(\n",
    "#             documents=texts,\n",
    "#             embedding=embeds,\n",
    "#             persist_directory=persist_directory\n",
    "#         )\n",
    "        \n",
    "#         texts = []\n",
    "#         metadatas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f5f9e-69ca-4912-92cd-01e01514352a",
   "metadata": {},
   "source": [
    "## Query vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf4da79f-0409-4203-b669-6dc7d9419a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed)\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6839f63-d6db-47ec-a1bf-41bb661da72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(i) “Electric Vehicle Charging” is defined as Card Transactions classified under the MCC 5552  \\n(Electric Vehicle Charging).  \\n \\nB. 0.25% cashback on the following transactions:  \\n• all Card Transactions if the Minimum Spend Requirement is not met,  \\n• all Card Transaction s (excluding Card Transactions under the categories listed at Clause \\n2A) if the Minimum Spend Requirement is met; and  \\n• Card Transactions expressly stated in Clause 2A(a)(iii) above.  \\n  \\n3. Minimum Spend Requirement  \\n \\na) The Minimum Spend Requirement must be met to receive the Cashback listed at Clause 2A \\nabove. There is no minimum spend requirement to receive the Base Cashback. The Minimum \\nSpend Requirement can be aggregated between the Principal and Supplementary Cardmembers. \\nThe maximum amount of Cashback t hat one account (the Principal and all Supplementary \\nCardmembers together) can earn in any calendar month is S$160.  \\n \\nMinimum Spend \\nRequirement  Cashback Cap', metadata={'source': '../docs/tncs-365cc-programme.pdf', 'page': 2}),\n",
       " Document(page_content='Account by the Principal and Supplementary Cardmembers in each calendar month , but \\nexcludes the Exclusio ns.  \\n \\nc) “Cashback” refers to the cashback awarded every calendar month under Clause 2 below.  \\n \\nd) “Exclusions” refers to the Card Transactions described under Clause 3(b) below.  \\n \\ne) “Minimum Spend Requirement” refers to all minimum spend requirements set out under Clause \\n3(a) below.  \\n \\n2. Cashback  \\n \\nThe OCBC 365 Card shall come with the following features:  \\n \\nA. Subject to the Minimum Spend Requirement  being met:  \\n \\na) 5% cashback on “Dining” Card Transactions  \\n \\n(i) “Dining” is defined as Card Transactions made in at all restaurants & cafes, caterers and \\nfast food restaurants classified under the Merchant Category Code  (“MCC”):  \\n• MCC 5812 (Restaurants and eating  places);  \\n• MCC 5814 (Fast Food Restaurants);  and \\n• MCC 5811  (Caterers).  \\n \\n(ii)  Cashback on Dining includes  :', metadata={'source': '../docs/tncs-365cc-programme.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the minimum spend?\"\n",
    "\n",
    "vectorstore.similarity_search(query, # our search query\n",
    "                              k=3 # return 3 most relevant docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc2124-fcda-4e1e-ae32-8e347ff2fa8a",
   "metadata": {},
   "source": [
    "## Generative QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "db8893e0-97d6-4739-8d10-ecc719e34e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The minimum spend requirement is the amount of money that needs to be spent on the OCBC 365 Card in order to qualify for certain cashback rewards. The specific amount of the minimum spend requirement is not mentioned in the given context.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a330ea26-68b7-449b-b262-21668bcaea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Are there any dining rewards?',\n",
       " 'answer': 'Yes, there are dining rewards. The OCBC 365 Card offers 5% cashback on \"Dining\" card transactions, which includes transactions made at restaurants, cafes, caterers, and fast food restaurants. However, transactions made at hotels, wedding banquets, and transactions not made in person (such as telephone or mail orders) are not eligible for the 5% cashback. \\n',\n",
       " 'sources': '../docs/tncs-365cc-programme.pdf'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
    "                                                              retriever=vectorstore.as_retriever()\n",
    "                                                             )\n",
    "\n",
    "query = \"Are there any dining rewards?\"\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba896-df2b-4bfe-8977-83be0f0cd02c",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ced386-b6ae-4950-95ed-44315d80b2f3",
   "metadata": {},
   "source": [
    "We can think of agents as enabling “tools” for LLMs. Like how a human would use a calculator for maths or perform a Google search for information — agents allow an LLM to do the same thing.\n",
    "\n",
    "Agents are LLMs that can use tools like calculators, search, or executing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b765568e-6530-4d2e-92b7-adb6fa731629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trucvietle/miniforge3/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import Tool\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "llm_math = LLMMathChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eec7fb3c-6867-4a7b-b378-917b4a1cbd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Calculator', 'Useful for when you need to answer questions about math.')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the math tool\n",
    "math_tool = Tool(name='Calculator',\n",
    "                 func=llm_math.run,\n",
    "                 description='Useful for when you need to answer questions about math.')\n",
    "\n",
    "# when giving tools to LLM, we must pass as list of tools\n",
    "tools = [math_tool]\n",
    "tools[0].name, tools[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7bd52ea7-cc78-4344-b550-94591e3e295a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Calculator', 'Useful for when you need to answer questions about math.')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the prebuilt math tool instead\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(['llm-math'], llm=llm)\n",
    "tools[0].name, tools[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4e9882c4-e7a4-48d7-b553-fe357a16a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "zero_shot_agent = initialize_agent(agent=\"zero-shot-react-description\",\n",
    "                                   tools=tools, llm=llm, verbose=True,\n",
    "                                   max_iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b09ba-d025-4d9e-863e-aaab1f3658b2",
   "metadata": {},
   "source": [
    "**Zero-shot** means the agent functions on the current action only — it has no memory. It uses the **ReAct framework** to decide which tool to use, based solely on the tool’s description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "42463e4c-5f22-48db-a584-f95c83fab797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to calculate this expression\n",
      "Action: Calculator\n",
      "Action Input: (4.5*2.1)^2.2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 139.94261298333066\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 139.94261298333066\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is (4.5*2.1)^2.2?', 'output': '139.94261298333066'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"What is (4.5*2.1)^2.2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6d7cacbc-929e-4831-b467-f91e6499b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to figure out how many apples are in the boxes and subtract the spoiled apples.\n",
      "Action: Calculator\n",
      "Action Input: 4 + (2.5 * 8)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 24.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to subtract the spoiled apples.\n",
      "Action: Calculator\n",
      "Action Input: 24.0 - (2.5 * 4)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 14.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: We have 14 apples in total.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If Mary has four apples and Joe brings two and a half apple boxes (apple box contains eight apples), but half of the apples in the boxes are spoiled and thrown away. How many apples do we have in total?',\n",
       " 'output': 'We have 14 apples in total.'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"If Mary has four apples and Joe brings two and a half apple boxes (each box contains eight apples), \\\n",
    "but half of the apples in the boxes are spoiled and thrown away. How many apples do we have in total?\"\n",
    "\n",
    "zero_shot_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ba79cd4f-1ed1-4d63-8588-4eedc949380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the Queen of Pop?\"\n",
    "# zero_shot_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e79e54c4-612e-42af-8c51-e8cef5a124c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"query\"], template=\"{query}\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# initialize the LLM tool\n",
    "llm_tool = Tool(name='Language Model',\n",
    "                func=llm_chain.run,\n",
    "                description='Use this tool for general purpose queries and logic')\n",
    "tools.append(llm_tool)\n",
    "\n",
    "# reinitialize the agent\n",
    "zero_shot_agent = initialize_agent(agent=\"zero-shot-react-description\",\n",
    "                                   tools=tools, llm=llm, verbose=True,\n",
    "                                   max_iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1bd2b1c6-755f-49fc-931a-dfb0c664176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who is considered the Queen of Pop.\n",
      "Action: Language Model\n",
      "Action Input: Who is the Queen of Pop?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "\n",
      "The Queen of Pop is generally considered to be Madonna.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Madonna is the Queen of Pop.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is the Queen of Pop?', 'output': 'Madonna is the Queen of Pop.'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0802422-0a54-4bc7-b2b7-8fb272b19426",
   "metadata": {},
   "source": [
    "## Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d03a5f-2c7a-4213-9073-0be5378570b9",
   "metadata": {},
   "source": [
    "- **Zero-shot ReAct** -- the agent considers one single interaction and has no memory.\n",
    "- **Conversational ReAct** -- same as Zero-shot ReAct but **with memory**.\n",
    "- **ReAct Docstore** -- explicitly built for information retrieval using a LangChain docstore. This agent is memoryless.\n",
    "- **Self-ask with Search** -- connect an LLM to a search engine (`SerpAPIWrapper`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64a5523d-6259-454b-a6d2-8473f536ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "conversational_agent = initialize_agent(agent='conversational-react-description', \n",
    "                                        tools=tools, llm=llm, verbose=True,\n",
    "                                        max_iterations=3, memory=memory)\n",
    "\n",
    "# result = conversational_agent(\"What is the stock price of IBM on January 2nd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a32b164d-7dc8-4284-b919-63c0004cd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using memory\n",
    "# result = conversational_agent(\"What is the stock price of SAP on the same day?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4cb0fe13-f394-417d-9058-d974ef525112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "\n",
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "\n",
    "tools = [Tool(name=\"Search\", func=docstore.search, description='Search wikipedia'),\n",
    "         Tool(name=\"Lookup\", func=docstore.lookup, description='Lookup a term in wikipedia')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "abeca943-d629-4e4b-ad25-97b657905d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search Archimedes and find his last words.\n",
      "Action: Search[Archimedes]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArchimedes of Syracuse (; c. 287 – c. 212 BC) was an Ancient Greek mathematician, physicist, engineer, astronomer, and inventor from the ancient city of Syracuse in Sicily. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity. Considered the greatest mathematician of ancient history, and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying the concept of the infinitely small and the method of exhaustion to derive and rigorously prove a range of geometrical theorems. These include the area of a circle, the surface area and volume of a sphere, the area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral.Archimedes' other mathematical achievements include deriving an approximation of pi, defining and investigating the Archimedean spiral, and devising a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, working on statics and hydrostatics. Archimedes' achievements in this area include a proof of the law of the lever, the widespread use of the concept of center of gravity, and the enunciation of the law of buoyancy or Archimedes' principle. He is also credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.\n",
      "Archimedes died during the siege of Syracuse, when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting Archimedes' tomb, which was surmounted by a sphere and a cylinder that Archimedes requested be placed there to represent his mathematical discoveries.\n",
      "Unlike his inventions, Archimedes' mathematical writings were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until c. 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes by Eutocius in the 6th century opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance and again in the 17th century, while the discovery in 1906 of previously lost works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paragraph does not mention Archimedes' last words. I need to look up \"last words\".\n",
      "Action: Lookup[last words]\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m(Result 1/1) \n",
      "== Biography ==\n",
      "Archimedes was born c. 287 BC in the seaport city of Syracuse, Sicily, at that time a self-governing colony in Magna Graecia. The date of birth is based on a statement by the Byzantine Greek historian John Tzetzes that Archimedes lived for 75 years before his death in 212 BC. In the Sand-Reckoner, Archimedes gives his father's name as Phidias, an astronomer about whom nothing else is known. A biography of Archimedes was written by his friend Heracleides, but this work has been lost, leaving the details of his life obscure. It is unknown, for instance, whether he ever married or had children, or if he ever visited Alexandria, Egypt, during his youth. From his surviving written works, it is clear that he maintained collegiate relations with scholars based there, including his friend Conon of Samos and the head librarian Eratosthenes of Cyrene.The standard versions of Archimedes' life were written long after his death by Greek and Roman historians. The earliest reference to Archimedes occurs in The Histories by Polybius (c. 200–118 BC), written about 70 years after his death. It sheds little light on Archimedes as a person, and focuses on the war machines that he is said to have built in order to defend the city from the Romans. Polybius remarks how, during the Second Punic War, Syracuse switched allegiances from Rome to Carthage, resulting in a military campaign under the command of Marcus Claudius Marcellus and Appius Claudius Pulcher, who besieged the city from 213 to 212 BC. He notes that the Romans underestimated Syracuse's defenses, and mentions several machines Archimedes designed, including improved catapults, crane-like machines that could be swung around in an arc, and other stone-throwers. Although the Romans ultimately captured the city, they suffered considerable losses due to Archimedes' inventiveness.Cicero (106–43 BC) mentions Archimedes in some of his works. While serving as a quaestor in Sicily, Cicero found what was presumed to be Archimedes' tomb near the Agrigentine gate in Syracuse, in a neglected condition and overgrown with bushes. Cicero had the tomb cleaned up and was able to see the carving and read some of the verses that had been added as an inscription. The tomb carried a sculpture illustrating Archimedes' favorite mathematical proof, that the volume and surface area of the sphere are two-thirds that of an enclosing cylinder including its bases. He also mentions that Marcellus brought to Rome two planetariums Archimedes built. The Roman historian Livy (59 BC–17 AD) retells Polybius' story of the capture of Syracuse and Archimedes' role in it.\n",
      "Plutarch (45–119 AD) wrote in his Parallel Lives that Archimedes was related to King Hiero II, the ruler of Syracuse. He also provides at least two accounts on how Archimedes died after the city was taken. According to the most popular account, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet Marcellus, but he declined, saying that he had to finish working on the problem. This enraged the soldier, who killed Archimedes with his sword. Another story has Archimedes carrying mathematical instruments before being killed because a soldier thought they were valuable items. Marcellus was reportedly angered by Archimedes' death, as he considered him a valuable scientific asset (he called Archimedes \"a geometrical Briareus\") and had ordered that he should not be harmed.The last words attributed to Archimedes are \"Do not disturb my circles\" (Latin, \"Noli turbare circulos meos\"; Katharevousa Greek, \"μὴ μου τοὺς κύκλους τάραττε\"), a reference to the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. There is no reliable evidence that Archimedes uttered these words and they do not appear in Plutarch's account. A similar quotation is found in the work of Valerius Maximus (fl. 30 AD), who wrote in Memorable Doings and Sayings, \"... sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare'\" (\"... but protecting the dust with his hands, said 'I beg of you, do not disturb this'\").\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paragraph mentions the last words attributed to Archimedes are \"Do not disturb my circles\". So the answer is \"Do not disturb my circles\".\n",
      "Action: Finish[\"Do not disturb my circles\"]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What were Archimedes' last words?\",\n",
       " 'output': '\"Do not disturb my circles\"'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore_agent = initialize_agent(tools, llm, agent=\"react-docstore\",\n",
    "                                  verbose=True, max_iterations=3)\n",
    "\n",
    "docstore_agent(\"What were Archimedes' last words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f8b30c5e-90d4-4817-a5db-7193952c3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "# initialize the search chain\n",
    "search = SerpAPIWrapper(serpapi_api_key='serp_api_key')\n",
    "\n",
    "# create a search tool\n",
    "tools = [Tool(name=\"Intermediate Answer\", func=search.run, description='Google search')]\n",
    "\n",
    "# initialize the search enabled agent\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "70e14c32-7cbd-4ca3-8384-771ff42623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_ask_with_search(\"Who died younger: Plato, Socrates, or Aristotle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4131625-8c5e-43c2-b354-eacc72937d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
