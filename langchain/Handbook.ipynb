{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd2c83ab-5858-460c-a9c2-ec36bffaa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"env_vars.env\")) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f221205-c498-406e-9d21-65255172aa60",
   "metadata": {},
   "source": [
    "# Hugging Face Hub LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c992cb-f75d-4b60-8e31-886656ff865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] output_parser=None partial_variables={} template='Question: {question} Answer: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question} Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "print(prompt)\n",
    "\n",
    "# Initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(repo_id='google/flan-t5-xl', model_kwargs={'temperature':1e-10})\n",
    "\n",
    "# Create prompt template > LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1f9fad3-8f54-4f68-b796-e8019495e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the current chairman of the Federal Reserve?\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "question = \"Who is the current chairman of the Federal Reserve?\"\n",
    "print(question)\n",
    "\n",
    "# NOTE: This will likely time out!\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462b959-9df1-49d9-bcb4-9d5416f7d039",
   "metadata": {},
   "source": [
    "# OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "826585c2-cb7e-4e96-867a-e5640b24c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the model\n",
    "davinci = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad2e5f5d-0214-4f58-8adf-b724370e2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The current chairman of the Federal Reserve is Jerome Powell.\n"
     ]
    }
   ],
   "source": [
    "# Ask the question\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f8b5b-8254-4be0-96f1-1cef23b18a71",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453f710-c041-4d93-a5ce-94e410b86f79",
   "metadata": {},
   "source": [
    "Two kinds of knowledge that can be \"reasoned\" by LLM:\n",
    "- **Parametric knowlege** -- learned during training and stored in model parameters (weights)\n",
    "- **Source knowledge** -- provided to the model at inference time as examples (via input prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11282157-da27-4105-9504-77707050db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The meaning of life is to find your own purpose and make the most of it. Life is what you make it!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: What is the meaning of life? AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0 # increase creativity/randomness of output\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e59ae676-c1a4-40d8-bac2-1efc3dcccea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The meaning of life is to live it to the fullest and enjoy every moment.\n"
     ]
    }
   ],
   "source": [
    "# To help the model, we can give it a few examples of the type of answers we’d like\n",
    "prompt = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: User: How are you? AI: I can't complain but sometimes I still do. \n",
    "User: What time is it? AI: It's time to get a watch. User: What is the meaning of life? AI: \"\"\"\n",
    "\n",
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371fe9f8-4c85-4e09-a1c3-0cdf1be359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate caters to source knowledge input\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\" User: {query} AI: {answer} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd45bcb-b5df-46c1-8638-175ce7cfed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query', 'answer'] output_parser=None partial_variables={} template=' User: {query} AI: {answer} ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(input_variables=[\"query\", \"answer\"], template=example_template)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af5bc184-eaa4-4bf4-a473-d15458a1857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}] example_selector=None example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI assistant. \n",
    "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
    "Here are some examples: \"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\" User: {query} AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples, example_prompt=example_prompt,\n",
    "                                                 prefix=prefix, suffix=suffix, input_variables=[\"query\"],\n",
    "                                                 example_separator=\"\\n\\n\")\n",
    "print(few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d17b7db-1546-4543-910b-489f4bcd623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      "\n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      "\n",
      " User: What time is it? AI: It's time to get a watch. \n",
      "\n",
      " User: What is the meaning of life? AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f31ec6c-cb32-48c3-9597-7a256638ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "affb28f2-d05c-484e-88a4-70aa386c128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}]\n"
     ]
    }
   ],
   "source": [
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed46cd6-2795-4550-8d09-14ebbb3c8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}] example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) get_text_length=<function _get_length_based at 0x11b59e3b0> max_length=50 example_text_lengths=[15, 14, 11, 16, 10, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(examples=examples,\n",
    "                                              example_prompt=example_prompt,\n",
    "                                              max_length=50 # this sets the max length that examples should be\n",
    "                                             )\n",
    "print(example_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f47011ae-51b3-4fd3-ae38-e90b2011a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] output_parser=None partial_variables={} examples=None example_selector=LengthBasedExampleSelector(examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do.\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch.\"}, {'query': 'What is the meaning of life?', 'answer': '42'}, {'query': 'What is the weather like today?', 'answer': 'Cloudy with a chance of memes.'}, {'query': 'What is your favorite movie?', 'answer': 'Terminator'}, {'query': 'Who is your best friend?', 'answer': 'Siri. We have spirited debates about the meaning of life.'}, {'query': 'What should I do today?', 'answer': 'Stop talking to chatbots on the internet and go outside.'}], example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True), get_text_length=<function _get_length_based at 0x11b59e3b0>, max_length=50, example_text_lengths=[15, 14, 11, 16, 10, 19, 19]) example_prompt=PromptTemplate(input_variables=['query', 'answer'], output_parser=None, partial_variables={}, template=' User: {query} AI: {answer} ', template_format='f-string', validate_template=True) suffix=' User: {query} AI: ' example_separator='\\n' prefix='The following are exerpts from conversations with an AI assistant. \\nThe assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \\nHere are some examples: ' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "print(dynamic_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eb4caf4-3121-4861-b612-fad860bd007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI assistant. \n",
      "The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions. \n",
      "Here are some examples: \n",
      " User: How are you? AI: I can't complain but sometimes I still do. \n",
      " User: What time is it? AI: It's time to get a watch. \n",
      " User: What is the meaning of life? AI: 42 \n",
      " User: How do birds fly? AI: \n"
     ]
    }
   ],
   "source": [
    "prompt = dynamic_prompt_template.format(query=\"How do birds fly?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcc4db1-a3f9-48b4-a169-07b8f3a4e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With a little help from their friends!\n"
     ]
    }
   ],
   "source": [
    "print(davinci(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fa83-d1a5-40b5-8866-085819ae3d59",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f6ca07-ac51-4ce8-9c79-914dabbc8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8998c1-e423-4d96-abae-badb0e7fdb99",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c754fb0-e597-474a-950d-3dd06b909325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fbfbf6c-e315-4882-9589-e3ecff44bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10361c19-f7d6-4194-abce-9fc4d094139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf,\n",
    "             \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5c2b73-dc41-416c-9f6f-ed4a8d69291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a4cab-a335-4114-af59-6fcff7378fe6",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f03f0c1-79ad-4a38-aa7a-5aa76bd6a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_sum = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88f5c3bb-8eb6-40b5-8cef-b256f7771796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(conversation_sum, \"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "211e33bc-a246-412e-a690-686f6668cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 383 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an exciting goal! What kind of steps do you think you need to take to achieve it?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"My interest here is to get filthy rich and live a lavish life!\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f620a2d-dad3-4d2d-ade5-13d0d9332a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 539 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a great goal! I think the first step is to identify what resources you have available to you. Do you have any savings, investments, or other assets that you can use to help you reach your goal?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_sum, \n",
    "             \"I just want to analyze the different possibilities to that goal. What can you think of?\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57f592ea-ee56-4e77-8dcd-011afcdc1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI and the AI responded with a greeting and asked how it could help. The human expressed their goal of getting rich and living a lavish life, to which the AI responded positively and asked what steps the human thought they needed to take to achieve it. The human asked the AI to analyze the different possibilities to that goal, and the AI suggested the first step should be to identify what resources the human had available to them.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7c51e-b86c-4460-a8da-a65180487c38",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a5c72c2-088e-4f7e-8919-65aaec726e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acts in the same way as our earlier “buffer memory” but adds a window to the memory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# We set k=1 -- meaning the window will remember the single latest interaction between the human and AI\n",
    "conversation_bufw = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "437b37c8-ab12-4435-b6b7-74a6abf1b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(inputs=[])['history']\n",
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02631ea7-2246-4eb8-bd3b-70609ae11ee6",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20f9e2d0-2a81-419f-9f0e-8509f9dba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes the earliest interactions in a conversation while maintaining the most recent tokens in the conversation\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation_sum_bufw = ConversationChain(llm=llm, \n",
    "                                          memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e894f6-cd02-46c5-a993-102e2084bdb4",
   "metadata": {},
   "source": [
    "This the only one of our memory types (so far) that allows us to remember distant interactions and store the most recent interactions in their raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ff9da27-a6c2-47ee-9ec3-ec8661a8771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others:\n",
    "# Knowledge graph\n",
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "\n",
    "# Entities -- remember details about the entities (persons, etc.)\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda678c-47de-4827-9ced-599419956777",
   "metadata": {},
   "source": [
    "# Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd70ee9-ff46-47fa-9fea-f37b9846f5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
