{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac959c4b-d7a9-4949-9c39-2ab46fbf82cd",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df1e2b-2079-4dee-84e9-77e13dd31e05",
   "metadata": {},
   "source": [
    "### Getting started with Llama 2\n",
    "\n",
    "The code to call the Llama 2 models through the Together.ai hosted API service has been wrapped into a helper function called `llama`. You can take a look at this code if you like by opening the utils.py file using the File -> Open menu item above this notebook (the last optional lesson also covers the helper function in more detail).\n",
    "\n",
    "Note: To see how to run Llama 2 locally on your own computer, you can go to the last section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10075542-6019-40b6-b3c1-532383e4a6dd",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# import llama helper function\n",
    "from utils import llama\n",
    "\n",
    "# # For loading the local model\n",
    "# from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820af7e0-6d0f-4135-bae7-8defd6de4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the local model\n",
    "# model_path = \"/Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "# model = Llama(model_path = model_path,\n",
    "#               n_ctx = 2048,            # context window size\n",
    "#               n_gpu_layers = 1,        # enable GPU\n",
    "#               use_mlock = True)        # enable memory lock so not swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a549fd-dd50-4ae1-a5d8-ce00fdc707a3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# define the prompt\n",
    "prompt = \"Help me write a birthday card for my boyfriend Guillaume.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d46c4b-2926-44a9-9e25-befb6bd6648c",
   "metadata": {},
   "source": [
    "**Note:** LLMs can have different responses for the same prompt, which is why throughout the course, the responses you get might be slightly different than the ones in the lecture videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44267891-7a9c-43a2-b757-921a6c4441db",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you write a birthday card for your boyfriend Guillaume! Here are a few suggestions:\n",
      "\n",
      "1. Personalized Message: Start by writing a personalized message that speaks directly to Guillaume. You could mention something specific that you love about him, a shared memory, or a quality that makes him special.\n",
      "\n",
      "Example: \"Happy birthday to the love of my life, Guillaume! You bring so much joy and happiness into my world, and I'm so grateful to have you by my side. I love your kind heart, your sense of humor, and the way you always know how to make me feel special. Here's to another amazing year together! ðŸŽ‰ðŸŽ‚\"\n",
      "\n",
      "2. Funny Quote: If you want to add a bit of humor to your card, consider using a funny quote that relates to Guillaume or your relationship.\n",
      "\n",
      "Example: \"Happy birthday to the man who makes my heart skip a beat... and my stomach laugh with his jokes! ðŸ˜‚ Here's to another year of loving, laughing, and making memories together. ðŸŽ‰ðŸŽ‚\"\n",
      "\n",
      "3. Heartfelt Message: If you want to write a more heartfelt message, focus on the emotions and feelings that Guillaume evokes in you.\n",
      "\n",
      "Example: \"As I wish you a happy birthday, I'm filled with so much love and gratitude. You're the sunshine that brightens up my day, the calm in every storm, and the safe haven where I can always find peace. I'm so lucky to have you in my life, Guillaume. Here's to another year of love, laughter, and adventures together! ðŸ’•ðŸŽ‰\"\n",
      "\n",
      "4. Memory Lane: Take a trip down memory lane and share a special memory or moment that you've shared with Guillaume.\n",
      "\n",
      "Example: \"Happy birthday to my favorite travel buddy, Guillaume! I'll never forget the time we explored the streets of Paris together, hand in hand. You make every moment special, and I'm so grateful for the memories we've created together. Here's to many more adventures and memories to come! ðŸ’•ðŸŽ‰\"\n",
      "\n",
      "5. Future Plans: If you want to look ahead to the future, consider writing a message that expresses your hopes and dreams for your relationship.\n",
      "\n",
      "Example: \"As you celebrate another year of life, I can't help but think about all the adventures we have ahead of us. I dream of the trips we'll take, the memories we'll make, and the love we'll share. You're my partner in every sense of the word, Guillaume, and I'm so excited to see what the future holds for us. Happy birthday, my love! ðŸ’•ðŸŽ‰\"\n",
      "\n",
      "Remember, the most important thing is to speak from your heart and be genuine. Your message should reflect your personality and relationship with Guillaume, so don't be afraid to add your own personal touches and anecdotes. Happy writing!\n"
     ]
    }
   ],
   "source": [
    "# pass prompt to the llama function, store output as 'response' then print\n",
    "response = llama(prompt)\n",
    "print(response)\n",
    "\n",
    "# output = model(prompt = prompt, max_tokens = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcfa140-fd31-49b1-b821-4ec05c8d1297",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "# print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573ca3f-9ffc-4f08-91eb-7e88d99d2f9f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Set verbose to True to see the full prompt that is passed to the model.\n",
    "# response = llama(prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c9bc9-9777-45bb-9e52-1cd99a42e716",
   "metadata": {},
   "source": [
    "### Chat vs. base models\n",
    "\n",
    "Ask model a simple question to demonstrate the different behavior of chat vs. base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37cc261-a9d5-486c-a0f1-7f0ce93f403e",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]What is the capital of France?[/INST]\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "### chat model\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"togethercomputer/llama-2-7b-chat\")\n",
    "\n",
    "# output = model(prompt = prompt, max_tokens = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d596d0-9469-4220-a03a-b50b0226776d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "\n",
    "# output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "# print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bef6df-ce83-4be7-98a7-cd0c353c4188",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "model: togethercomputer/llama-2-7b\n"
     ]
    }
   ],
   "source": [
    "### base model\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "response = llama(prompt,\n",
    "                 verbose=True,\n",
    "                 add_inst=False,\n",
    "                 model=\"togethercomputer/llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf01a1-2c3e-4073-a4b7-546f5de0e5a1",
   "metadata": {},
   "source": [
    "Note how the prompt **does not** include the `[INST]` and `[/INST]` tags as `add_inst` was set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1941be3a-de66-4d08-808e-f93a0393f864",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. What is the capital of Germany?\n",
      "11. What is the capital of Greece?\n",
      "12. What is the capital of Hungary?\n",
      "13. What is the capital of Iceland?\n",
      "14. What is the capital of India?\n",
      "15. What is the capital of Indonesia?\n",
      "16. What is the capital of Iran?\n",
      "17. What is the capital of Iraq?\n",
      "18. What is the capital of Ireland?\n",
      "19. What is the capital of Israel?\n",
      "20. What is the capital of Italy?\n",
      "21. What is the capital of Japan?\n",
      "22. What is the capital of Jordan?\n",
      "23. What is the capital of Kazakhstan?\n",
      "24. What is the capital of Kenya?\n",
      "25. What is the capital of Kuwait?\n",
      "26. What is the capital of Kyrgyzstan?\n",
      "27. What is the capital of Laos?\n",
      "28. What is the capital of Latvia?\n",
      "29. What is the capital of Lebanon?\n",
      "30. What is the capital of Lesotho?\n",
      "31. What is the capital of Liberia?\n",
      "32. What is the capital of Libya?\n",
      "33. What is the capital of Liechtenstein?\n",
      "34. What is the capital of Lithuania?\n",
      "35. What is the capital of Luxembourg?\n",
      "36. What is the capital of Macedonia?\n",
      "37. What is the capital of Madagascar?\n",
      "38. What is the capital of Malawi?\n",
      "39. What is the capital of Malaysia?\n",
      "40. What is the capital of Maldives?\n",
      "41. What is the capital of Mali?\n",
      "42. What is the capital of Malta?\n",
      "43. What is the capital of Marshall Islands?\n",
      "44. What is the capital of Mauritania?\n",
      "45. What is the capital of Mauritius?\n",
      "46. What is the capital of Mexico?\n",
      "47. What is the capital of Micronesia?\n",
      "48. What is the capital of Moldova?\n",
      "49. What is the capital of Monaco?\n",
      "50. What is the capital of Mongolia?\n",
      "51. What is the capital of Montenegro?\n",
      "52. What is the capital of Morocco?\n",
      "53. What is the capital of Mozambique?\n",
      "54. What is the capital of Myanmar?\n",
      "55. What is the capital of Namibia?\n",
      "56. What is the capital of Nauru?\n",
      "57. What is the capital of Nepal?\n",
      "58. What is the capital of Netherlands?\n",
      "59. What is the capital of New Zealand?\n",
      "60. What is the capital of Nicaragua?\n",
      "61. What is the capital of Niger?\n",
      "62. What is the capital of Nigeria?\n",
      "63. What is the capital of Norway?\n",
      "64. What is the capital of Oman?\n",
      "65. What is the capital of Pakistan?\n",
      "66. What is the capital of Palau?\n",
      "67. What is the capital of Palestine?\n",
      "68. What is the capital of Panama?\n",
      "69. What is the capital of Papua New Guinea?\n",
      "70. What is the capital of Paraguay?\n",
      "71. What is the capital of Peru?\n",
      "72. What is the capital of Philippines?\n",
      "73. What is the capital of Poland?\n",
      "74. What is the capital of Portugal?\n",
      "75. What is the capital of Qatar?\n",
      "76. What is the capital of Romania?\n",
      "77. What is the capital of Russia?\n",
      "78. What is the capital of Rwanda?\n",
      "79. What is the capital of Saint Kitts and Nevis?\n",
      "80. What is the capital of Saint Lucia?\n",
      "81. What is the capital of Saint Vincent and the Grenadines?\n",
      "82. What is the capital of Samoa?\n",
      "83. What is the capital of San Marino?\n",
      "84. What is the capital of Sao Tome and Principe?\n",
      "85. What is the capital of Saudi Arabia?\n",
      "86. What is the capital of Senegal?\n",
      "87. What is the capital of Serbia?\n",
      "88. What is the capital of Seychelles?\n",
      "89. What is the capital of Sierra Leone?\n",
      "90. What is the capital of Singapore?\n",
      "91. What is the capital of Slovakia?\n",
      "92. What is the capital of Sloven\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ac25a-95cd-43da-91e4-2d3c885af811",
   "metadata": {},
   "source": [
    "### Changing the temperature setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c11476-c128-4370-9405-e2899c0284ba",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you write a birthday card for your boyfriend Guillaume! Here's a suggestion:\n",
      "\n",
      "Dear Guillaume,\n",
      "\n",
      "Happy birthday to the love of my life! ðŸŽ‰\n",
      "\n",
      "I hope your day is filled with all your favorite things, including French politics, sustainability, and navy blue (your favorite color, of course!). ðŸŒŠ\n",
      "\n",
      "I'm so grateful to have you in my life, and I'm constantly inspired by your passion for making the world a better place. Your commitment to environmental sustainability and social justice is truly admirable, and I'm so lucky to be by your side as we work together towards a brighter future. ðŸŒŸ\n",
      "\n",
      "And speaking of cats, I hope you have a purrfect day with your feline friends! ðŸ˜º They bring so much joy and love into our lives, don't they? ðŸ’•\n",
      "\n",
      "Here's to another amazing year together, my love! ðŸ¥³ Cheers to you, Guillaume! ðŸ¥‚\n",
      "\n",
      "With all my love and appreciation,\n",
      "[Your Name] ðŸ’—\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear boyfriend Guillaume.\n",
    "Here are details about him:\n",
    "He likes to follow French politics.\n",
    "His cares about the environment and sustainability.\n",
    "His favorite color is navy.\n",
    "He likes cats.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt, temperature=0.0)\n",
    "print(response)\n",
    "\n",
    "# output = model(prompt=prompt, max_tokens=240)\n",
    "# output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a8ba4-ae6d-4855-bcbc-e492ce9119f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065d446f-2eef-4f34-858b-c5146e154cf0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# # Run the code again - the output should be identical\n",
    "# response = llama(prompt, temperature=0.0)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb83e70c-c629-470a-8013-cc63c56b5870",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you write a birthday card for your boyfriend Guillaume! Here's a suggestion:\n",
      "\n",
      "\"Happy birthday to my dear Guillaume! ðŸŽ‰\n",
      "\n",
      "As we celebrate another year of life, I can't help but think of how lucky I am to have you in my life. Your passion for French politics inspires me every day, and I admire your dedication to making the world a better place through sustainability. ðŸŒŽ\n",
      "\n",
      "Your love for navy blue is a classic touch, and I must say I'm a fan myself! ðŸ˜‰ And let's not forget your feline friends - they bring so much joy to your life, and to mine as well. ðŸ±\n",
      "\n",
      "With love and appreciation, happy birthday Guillaume! May your day be filled with all your favorite things and may your year ahead be filled with happiness, health, and adventure. ðŸŽ‚ðŸ¥³\"\n",
      "\n",
      "I hope this helps inspire you to write a heartfelt and personalized message to your boyfriend. Don't hesitate to modify it to fit your own voice and style!\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt, temperature=0.75)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6834c2db-16c7-46ed-8c79-9aac6041fe66",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here's a sample birthday card message for your boyfriend Guillaume:\n",
      "\n",
      "\"Happy birthday to my dear Guillaume! ðŸŽ‰\n",
      "\n",
      "I hope your day is as amazing as you are. I love that you're passionate about following French politics and doing your part to help the environment. It's so important to be mindful of our impact on the world around us. ðŸŒŽ\n",
      "\n",
      "I also adore your navy blue taste - it's such a classic and handsome color. ðŸ’™ And let's be real, who doesn't love cats? ðŸ˜¸ They're the ultimate stress-relievers and snuggle buddies.\n",
      "\n",
      "Here's to another incredible year together, filled with love, laughter, and all your favorite things. â¤ï¸\n",
      "\n",
      "Happy birthday, Guillaume! ðŸŽ‰\"\n",
      "\n",
      "I hope this helps inspire you to craft the perfect birthday card message for your boyfriend!\n"
     ]
    }
   ],
   "source": [
    "# run the code again - the output should be different\n",
    "response = llama(prompt, temperature=0.75)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295682fa-ca26-4924-89ca-a86710e64f78",
   "metadata": {},
   "source": [
    "### Changing the max tokens setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ede4dfa8-0d2c-42da-b588-701119bd136f",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you write a birthday card for your boy\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,max_tokens=20)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333a590-b54a-44f3-a627-1db0fe462315",
   "metadata": {},
   "source": [
    "The next cell reads in the text of the children's book *The Velveteen Rabbit* by Margery Williams, and stores it as a string named `text`. (Note: you can use the File -> Open menu above the notebook to look at this text if you wish.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0a6d79-8c32-4b56-869d-ec7b4b9e526c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with open(\"TheVelveteenRabbit.txt\", \"r\", encoding='utf=8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e31bf5-b323-4771-ae59-70b73e00ec4b",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f15ff3c-0068-4abe-8e6f-e3baf92dd31d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4097. Given: 3974 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0f2dc-6822-4636-92cd-d7cd77d9a27f",
   "metadata": {},
   "source": [
    "Running the cell above returns an error because we have too many tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04efbc3e-e67f-4300-a46d-7bf3834077dc",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of input tokens (prompt + Velveteen Rabbit text) and output tokens\n",
    "3974 + 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7ac3-24df-48da-8868-5cddd6176dfe",
   "metadata": {},
   "source": [
    "For Llama 2 chat models, the sum of the input and max_new_tokens parameter must be <= 4097 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7e1c8d-b16c-41af-8c83-79ce559f860a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate tokens available for response after accounting for 3974 input tokens\n",
    "4097 - 3974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e93887be-c3ea-44d2-86fc-a4aa9e101a37",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# set max_tokens to stay within limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e8b6db-e1ac-4480-bd86-d57334d6db57",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The Velveteen Rabbit is a heartwarming story about the relationship between a young boy and his stuffed toy rabbit. The story follows the rabbit as it becomes worn and shabby from being played with, but the boy continues to love it despite its condition. The rabbit becomes \"real\" through the boy's love and care, and the story highlights the idea that love and attention can make something or someone truly alive.\n",
      "\n",
      "The story is written in a simple and straightforward style, making it easy to follow and understand. The use of descriptive language\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ccbe1ce-35f6-4bdb-888a-5232b3a23794",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# increase max_tokens beyond limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1e6e7b-0932-4238-9dc1-93dc0891895b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4097. Given: 3974 `inputs` tokens and 124 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768b69e-ee6c-445f-b68b-a7480a76a019",
   "metadata": {},
   "source": [
    "### Asking a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b613eb4a-c922-4037-ad1e-ffb8cdea1c1f",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you write a birthday card for your boyfriend Guillaume! Here's a suggestion:\n",
      "\n",
      "Dear Guillaume,\n",
      "\n",
      "Happy birthday to the love of my life! ðŸŽ‰\n",
      "\n",
      "I hope your day is filled with all your favorite things, including French politics, sustainability, and navy blue (your favorite color, of course!). ðŸŒŠ\n",
      "\n",
      "I'm so grateful to have you in my life, and I'm constantly inspired by your passion for making the world a better place. Your commitment to environmental sustainability and social justice is truly admirable, and I'm so lucky to be by your side as we work together towards a brighter future. ðŸŒŸ\n",
      "\n",
      "And speaking of cats, I hope you have a purrfect day with your feline friends! ðŸ˜º They bring so much joy and love into our lives, don't they? ðŸ’•\n",
      "\n",
      "Here's to another amazing year together, my love! ðŸ¥³ Cheers to you, Guillaume! ðŸ¥‚\n",
      "\n",
      "With all my love and appreciation,\n",
      "[Your Name] ðŸ’—\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear boyfriend Guillaume.\n",
    "Here are details about him:\n",
    "He likes to follow French politics.\n",
    "His cares about the environment and sustainability.\n",
    "His favorite color is navy.\n",
    "He likes cats.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4333b1d1-e4fe-436a-a3af-d23eed09b866",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here's a revised version of the bio that includes his interest in traveling:\n",
      "\n",
      "\"John is a highly skilled software engineer with a passion for creating innovative solutions. He has a strong background in computer science and has worked on a wide range of projects, from developing complex algorithms to building intuitive user interfaces. In his free time, John enjoys traveling and exploring new cultures, which he believes has helped him to approach problems from unique perspectives and think outside the box. He is always looking for new challenges and opportunities to grow both personally and professionally, and he is excited to bring his skills and experience to the team at [Company Name].\"\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Oh, he also likes traveling. Can you rewrite it to include that?\n",
    "\"\"\"\n",
    "\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8355e-2da8-411d-8bf1-c0b545ddc4da",
   "metadata": {},
   "source": [
    "### (Optional): Using Llama-2 on your own computer!\n",
    "- The small 7B Llama chat model is free to download on your own machine!\n",
    "  - **Note** that only the Llama 2 7B chat model (by default the 4-bit quantized version is downloaded) may work fine locally.\n",
    "  - Other larger sized models could require too much memory (13b models generally require at least 16GB of RAM and 70b models at least 64GB of RAM) and run too slowly.\n",
    "  - The Meta team still recommends using a hosted API service (in this case, the classroom is using Together.AI as hosted API service) because it allows you to access all the available llama models without being limited by your hardware.\n",
    "  - You can find more instructions on using the Together.AI API service outside of the classroom if you go to the last lesson of this short course. \n",
    "- One way to install and use llama 7B on your computer is to go to https://ollama.com/ and download app. It will be like installing a regular application.\n",
    "- To use llama-2, the full instructions are here: https://ollama.com/library/llama2\n",
    "\n",
    "\n",
    "#### Here's an quick summary of how to get started:\n",
    "  - Follow the installation instructions (for Windows, Mac or Linux).\n",
    "  - Open the command line interface (CLI) and type `ollama run llama2`.\n",
    "  - The first time you do this, it will take some time to download the llama-2 model. After that, you'll see \n",
    "> `>>> Send a message (/? for help)`\n",
    "\n",
    "- You can type your prompt and the llama-2 model on your computer will give you a response!\n",
    "- To exit, type `/bye`.\n",
    "- For a list of other commands, type `/?`.\n",
    "\n",
    "![](ollama_example.png \"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a96fcfda-6424-43fc-bed5-ac55858f6c8f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2494cc97-b659-4e09-9961-8aa9b81aec32",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   123.81 MiB, (  123.88 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      Metal buffer size =   123.81 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  157.44 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  157.45 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   172.02 MiB, (  329.45 / 10922.67)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =   172.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   172.00 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "model = Llama(model_path = model_path,\n",
    "              n_ctx = 2048,            # context window size\n",
    "              n_gpu_layers = 1,        # enable GPU\n",
    "              use_mlock = True)        # enable memory lock so not swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3db37258-89de-41c0-a887-67c8dbd6db61",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4608.54 ms\n",
      "llama_print_timings:      sample time =      14.67 ms /   175 runs   (    0.08 ms per token, 11925.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4607.58 ms /   153 tokens (   30.11 ms per token,    33.21 tokens per second)\n",
      "llama_print_timings:        eval time =   10117.52 ms /   174 runs   (   58.15 ms per token,    17.20 tokens per second)\n",
      "llama_print_timings:       total time =   14997.33 ms /   327 tokens\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "Help me write a birthday card for my dear boyfriend Guillaume.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "output = model(prompt = prompt, max_tokens = 420, temperature = 0.2)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c90ea0e9-8f04-48d5-ade4-f640930996c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! I'd be happy to help you write a birthday card for your boyfriend Guillaume. Here are some suggestions:\n",
      "Dear Guillaume,\n",
      "Happy birthday to an amazing person! On this special day, I want to express how much you mean to me and how grateful I am to have you in my life. Your kindness, compassion, and love are just a few of the many qualities that make you such an incredible partner.\n",
      "As we celebrate your birthday, I hope you know how much you're appreciated and valued. You bring joy and happiness into my world, and I feel so lucky to have you by my side. Here's to another year of love, laughter, and adventures together!\n",
      "With all my love and appreciation,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357e23a-f206-4a1b-a0b5-24f783c0f438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
