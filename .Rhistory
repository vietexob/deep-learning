rm(list = ls())
actions <- c("N", "S", "E", "W")
## Gridworld is of dimension 3x4 ?
x <- 1:4
y <- 1:3
x
y
rewards <- matrix(rep(0, 12), nrow = 3)
rewards[2, 2] <- NA
rewards[1, 4] <- 1
rewards[2, 4] <- -1
rewards
x
y
values
# Set the initial values
values <- rewards
states <- expand.grid(x = x, y = y)
values
states
# Transition probability
transition <- list("N" = c("N" = 0.8, "S" = 0, "E" = 0.1, "W" = 0.1),
"S"= c("S" = 0.8, "N" = 0, "E" = 0.1, "W" = 0.1),
"E"= c("E" = 0.8, "W" = 0, "S" = 0.1, "N" = 0.1),
"W"= c("W" = 0.8, "E" = 0, "S" = 0.1, "N" = 0.1))
transition
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
action.values
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
action.values
rewards
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
transition
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
rewards
states
quit()
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
values
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
rewards
values
states
final.values
rewards
values
transition
values
states
?unlist
unlist(states[1, ])
states[1, ]
states[2, ]
unlist(states[2, ])
foo <- unlist(states[2, ])
foo[1]
fo[2]
foo[2]
length(foo)
states
nrow(states)
?lapply
q.values
q.values
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
actions
actions
foo <- c(NA, NA, NA)
max(foo)
foo <- c(NA, NA, 0)
max(foo)
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
rewards
values
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
foo <- c(NA, 1, 2, 3)
max(foo)
?max
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
warnings()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
states
foo <- expand.grid(x = y, y = x)
foo
states
typeof(states)
length(states)
nrow(states)
ncol(states)
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
actions
actions
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
actions
state
state
states
test
test = unlist(states[2, ])
test
foo <- lapply(actions, bellman.update, state=test, values=values, gamma=1)
foo
as.numeric(foo)
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
transition
values
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
quit()
library(mlbench)
install.packages('mlbench')
library(mlbench)
source('~/Documents/workspace/deep-learning/code/test.R')
summary(BostonHousing)
dim(BostonHousing)
summary(BostonHousing$medv)
source('~/Documents/workspace/deep-learning/code/test.R')
source('~/Documents/workspace/deep-learning/code/test.R')
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
library(nnet)
?nnet
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
source('~/Documents/workspace/deep-learning/code/test.R')
?nnet
source('~/Documents/workspace/deep-learning/code/test.R')
rm(list = ls())
library(mlbench)
library(nnet)
data("BostonHousing")
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
source('~/Documents/workspace/deep-learning/code/test.R')
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 2)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
library(caret)
my_grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(4, 5, 6))
nnet_fit <- train(medv / 50 ~ ., data = BostonHousing, method = 'nnet',
maxit = 1000, tuneGrid = my_grid, trace = FALSE)
print(nnet_fit)
?train
?expand.grid
my_grid
lm_fit <- train(medv / 50 ~ ., data = BostonHousing, method = 'lm')
print(lm_fit)
?nnet
dim(BostonHousing)
head(BostonHousing)
hist(BostonHousing$medv)
hist(BostonHousing$medv, breaks = 50)
hist(BostonHousing$medv, breaks = 20)
quit()
getwd()
rm(list = ls())
library(mlbench)
library(nnet)
library(caret)
data("BostonHousing")
## Model using neural network
## Scale the input to get 0-1 range
nnet_fit <- nnet(medv / 50 ~ ., data = BostonHousing, size = 3)
## Multiply 50 to restore the original scale
nnet_pred <- predict(nnet_fit) * 50
## Mean squared error
print(mean((nnet_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, nnet_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
## Optimize the NN hyperparams
my_grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(4, 5, 6))
nnet_fit <- train(medv / 50 ~ ., data = BostonHousing, method = 'nnet', maxit = 1000,
tuneGrid = my_grid, trace = FALSE)
print(nnet_fit)
lm_fit <- train(medv / 50 ~ ., data = BostonHousing, method = 'lm')
print(lm_fit)
?train
## Model using linear regression
lm_fit <- lm(medv ~ ., data = BostonHousing)
lm_pred <- predict(lm_fit)
## Mean squared error
print(mean((lm_pred - BostonHousing$medv)^2))
plot(BostonHousing$medv, lm_pred, main = 'LM Pred vs. Actual', xlab = 'Actual')
?nnet
quit()
getwd()
