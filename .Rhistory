rm(list = ls())
actions <- c("N", "S", "E", "W")
## Gridworld is of dimension 3x4 ?
x <- 1:4
y <- 1:3
x
y
rewards <- matrix(rep(0, 12), nrow = 3)
rewards[2, 2] <- NA
rewards[1, 4] <- 1
rewards[2, 4] <- -1
rewards
x
y
values
# Set the initial values
values <- rewards
states <- expand.grid(x = x, y = y)
values
states
# Transition probability
transition <- list("N" = c("N" = 0.8, "S" = 0, "E" = 0.1, "W" = 0.1),
"S"= c("S" = 0.8, "N" = 0, "E" = 0.1, "W" = 0.1),
"E"= c("E" = 0.8, "W" = 0, "S" = 0.1, "N" = 0.1),
"W"= c("W" = 0.8, "E" = 0, "S" = 0.1, "N" = 0.1))
transition
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
action.values
quit()
source('~/Documents/workspace/deep-learning/code/reinforcement/mdp.R')
action.values
rewards
quit()
source('C:/Users/trucviet.le.2012/workspace/deep-learning/code/reinforcement/mdp.R')
rewards
action.values
x
y
x[0]
x[1]
x
y
rewards
quit()
