{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4d690-6f97-4c1b-bf58-195242310d35",
   "metadata": {},
   "source": [
    "## RAG Demo Using Offline Docs\n",
    "\n",
    "### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020727ec-86ce-4c5a-a962-4a18451b64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"env_vars.env\")) # read local .env file\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2084c-2d5b-4d72-92fb-89c9c52c6589",
   "metadata": {},
   "source": [
    "### Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0ea9d8-89ce-4f9c-8a4b-16fc93554c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [PyPDFLoader(\"../docs/Inview_June_2023.pdfInview_June_2023.pdf\")]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e239bcb-376f-4020-afee-714406f83d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f02cd-3a11-4d94-a32a-a1fd24281643",
   "metadata": {},
   "source": [
    "### Store \"chunks\" as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4aae2d-e323-4115-be0f-e4a137c7e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The embedding libraries\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# persist_directory = '../docs/chroma-test/'\n",
    "# !rm -rf ../docs/chroma-test/  # remove old database files if any\n",
    "\n",
    "# # # OpenAI embedding\n",
    "# # embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# # # Try a different embedding (HF)\n",
    "# # embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# # Try a different embedding (ST)\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606e69e6-5ef5-40e8-9c2e-243e6d2c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store in the local vector DB\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "# print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda8f59-7935-4f5c-a1ba-ee07f56629cb",
   "metadata": {},
   "source": [
    "### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c81e1-6bd1-48d7-9f00-0c8b646014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What industry sectors are recommended for stocks?\"\n",
    "\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d23e6-b7e1-4ff7-ac3e-0ba430e8e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top-ranked result\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc318a-8764-45d6-b105-5bbf51564110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca13c49-6cd2-4be2-8226-83f6091232a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + \n",
    "                                   d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656eccd-e982-4b3b-b3a7-4a0b72bcfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d4e79-3e33-4d16-817f-fee8e516d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the vector DB for RAG\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bab50c-7f27-4a9a-8a1d-6b94ad62e5d1",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf8700b-1bfb-46d4-8810-dca19c3928ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Invoke the LLM\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b20f03-b1bc-4407-986b-14f76baca4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain and prompt template\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build the prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run the chain\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2e29d3-bd2a-4017-bb44-c3ee7c25d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "including local currency emerging market debt. Finally, \n",
      "to limit the riskiness of the portfolio, exposure to high \n",
      "yield bonds should be reduced in favour of investment \n",
      "grade corporate bonds.\n",
      "03 Global and Fixed Income Asset Allocation 04 Equity Sector Views\n",
      "04 Equities Asset Allocation 05 Alternatives Asset AllocationContents\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "that inflation is declining globally. This is partly thanks \n",
      "to the fall in the prices of raw materials. Furthermore, \n",
      "services prices are also starting to return to levels \n",
      "more consistent with central banks’ targets. The \n",
      "tightening of monetary policy also appears to be \n",
      "nearing completion in many parts of the world and \n",
      "this is supportive for markets.At the same time, the earnings season has been better \n",
      "than expected, especially for technology companies. \n",
      "The brighter prospects for corporate profitability in \n",
      "2023 are another favorable factor for the markets.\n",
      "The implications for the asset allocation of a \n",
      "diversified portfolio are that, in our view, a moderate \n",
      "overweight in both equities and bonds remains \n",
      "appropriate. However, some adjustments in the \n",
      "allocation within asset classes are advisable. Within \n",
      "equities, uncertainty around the Chinese recovery and \n",
      "its spillover into Europe argue for a trimming of the \n",
      "overweight in Asian equities, a reduction to neutral \n",
      "of the exposure to European markets and a neutral \n",
      "position on emerging Europe and the Middle East. In \n",
      "contrast, attractive valuations and solid performance \n",
      "of Japan equities warrant an overweight position. \n",
      "Among fixed income assets, increased bond yields and \n",
      "the nearing of the end of monetary policy tightening \n",
      "make longer-dated government bonds attractive, \n",
      "including local currency emerging market debt. Finally, \n",
      "to limit the riskiness of the portfolio, exposure to high\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "02  |  Inview June  2023  Welcome to the June edition of Inview: Monthly \n",
      "Global House View . In this publication we consider \n",
      "significant developments in the world’s markets, \n",
      "and discuss our key convictions and themes for \n",
      "the coming months.Editorial\n",
      "Moz Afzal\n",
      "Chief Investment OfficerEDITORIAL\n",
      "Global equity markets have been rangebound for some \n",
      "time and the trend continued in May with a 1.25% \n",
      "drop in the MSCI World index, most of which came \n",
      "on the last trading day of the month. Even if some \n",
      "elements of concern regarding the economic situation \n",
      "seem to have diminished, the risks remain mainly \n",
      "to the downside for growth. Nonetheless, corporate \n",
      "earnings beat expectations, especially for technology \n",
      "companies, reinforcing confidence that markets can \n",
      "continue to rally.\n",
      "For now, markets remain caught between two \n",
      "opposing forces. On one side of the debate, the loss of \n",
      "economic momentum in both developed and emerging \n",
      "countries is evident. Hopes that a strong recovery in \n",
      "China would support global growth have collided with \n",
      "the reality of a more modest reopening in the absence \n",
      "of significant economic policy stimulus. At the same \n",
      "time, tensions around the US debt ceiling and the \n",
      "repercussions of instability in the financial sector have \n",
      "made financial conditions more restrictive.\n",
      "On the other side of the debate, it is increasingly clear \n",
      "that inflation is declining globally. This is partly thanks \n",
      "to the fall in the prices of raw materials. Furthermore,\n"
     ]
    }
   ],
   "source": [
    "# question = \"What is the main topic of this document?\"\n",
    "# question = \"What industry sectors are recommended for stocks?\"\n",
    "question = \"What is the house view between stocks and bonds?\"\n",
    "\n",
    "# Print out the retrieved contexts\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(retrieved_docs)\n",
    "\n",
    "# result = qa_chain({\"query\": question})\n",
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b1cdf9b-64e7-4611-99ed-4cc2557dce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.    If you don't know the answer, just say that you don't know, don't try to make up an answer.    Use three sentences maximum. Keep the answer as concise as possible.\n",
      "\n",
      "Context:\n",
      "including local currency emerging market debt. Finally, \n",
      "to limit the riskiness of the portfolio, exposure to high \n",
      "yield bonds should be reduced in favour of investment \n",
      "grade corporate bonds.\n",
      "03 Global and Fixed Income Asset Allocation 04 Equity Sector Views\n",
      "04 Equities Asset Allocation 05 Alternatives Asset AllocationContents\n",
      "\n",
      "---\n",
      "\n",
      "that inflation is declining globally. This is partly thanks \n",
      "to the fall in the prices of raw materials. Furthermore, \n",
      "services prices are also starting to return to levels \n",
      "more consistent with central banks’ targets. The \n",
      "tightening of monetary policy also appears to be \n",
      "nearing completion in many parts of the world and \n",
      "this is supportive for markets.At the same time, the earnings season has been better \n",
      "than expected, especially for technology companies. \n",
      "The brighter prospects for corporate profitability in \n",
      "2023 are another favorable factor for the markets.\n",
      "The implications for the asset allocation of a \n",
      "diversified portfolio are that, in our view, a moderate \n",
      "overweight in both equities and bonds remains \n",
      "appropriate. However, some adjustments in the \n",
      "allocation within asset classes are advisable. Within \n",
      "equities, uncertainty around the Chinese recovery and \n",
      "its spillover into Europe argue for a trimming of the \n",
      "overweight in Asian equities, a reduction to neutral \n",
      "of the exposure to European markets and a neutral \n",
      "position on emerging Europe and the Middle East. In \n",
      "contrast, attractive valuations and solid performance \n",
      "of Japan equities warrant an overweight position. \n",
      "Among fixed income assets, increased bond yields and \n",
      "the nearing of the end of monetary policy tightening \n",
      "make longer-dated government bonds attractive, \n",
      "including local currency emerging market debt. Finally, \n",
      "to limit the riskiness of the portfolio, exposure to high\n",
      "\n",
      "---\n",
      "\n",
      "02  |  Inview June  2023  Welcome to the June edition of Inview: Monthly \n",
      "Global House View . In this publication we consider \n",
      "significant developments in the world’s markets, \n",
      "and discuss our key convictions and themes for \n",
      "the coming months.Editorial\n",
      "Moz Afzal\n",
      "Chief Investment OfficerEDITORIAL\n",
      "Global equity markets have been rangebound for some \n",
      "time and the trend continued in May with a 1.25% \n",
      "drop in the MSCI World index, most of which came \n",
      "on the last trading day of the month. Even if some \n",
      "elements of concern regarding the economic situation \n",
      "seem to have diminished, the risks remain mainly \n",
      "to the downside for growth. Nonetheless, corporate \n",
      "earnings beat expectations, especially for technology \n",
      "companies, reinforcing confidence that markets can \n",
      "continue to rally.\n",
      "For now, markets remain caught between two \n",
      "opposing forces. On one side of the debate, the loss of \n",
      "economic momentum in both developed and emerging \n",
      "countries is evident. Hopes that a strong recovery in \n",
      "China would support global growth have collided with \n",
      "the reality of a more modest reopening in the absence \n",
      "of significant economic policy stimulus. At the same \n",
      "time, tensions around the US debt ceiling and the \n",
      "repercussions of instability in the financial sector have \n",
      "made financial conditions more restrictive.\n",
      "On the other side of the debate, it is increasingly clear \n",
      "that inflation is declining globally. This is partly thanks \n",
      "to the fall in the prices of raw materials. Furthermore,\n",
      "\n",
      "Question: What is the house view between stocks and bonds?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "contexts = [d.page_content for d in retrieved_docs]\n",
    "\n",
    "prompt_start = (\n",
    "    \"Use the following pieces of context to answer the question at the end.\\\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\\n",
    "    Use three sentences maximum. Keep the answer as concise as possible.\\n\\n\"+\n",
    "    \"Context:\\n\"\n",
    ")\n",
    "\n",
    "prompt_end = (\n",
    "    f\"\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + \n",
    "    prompt_end\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da0ec715-ed3c-4e0b-8dc5-d1ef2b8886bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   247.62 MiB, ( 2258.62 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      Metal buffer size =   247.62 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3840.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 2514.62 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2514.64 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   568.02 MiB, ( 3082.64 / 10922.67)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =   568.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   568.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Try on local models\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# model_path = \"/Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf\"\n",
    "model_path = \"/Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_ctx=8192, n_batch=512,\n",
    "            n_threads=7, n_gpu_layers=2,\n",
    "            verbose=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b4638c-ad45-4a25-b48b-52783c55d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The house view is that a moderate overweight in both equities and bonds remains appropriate. However, some adjustments in the allocation within asset classes are advisable.\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt, echo=True, stream=False, max_tokens=4096)\n",
    "\n",
    "output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41095db-0e05-4274-8b5c-c59b93a74c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[\"source_documents\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d411f-20a7-4327-8f6f-eb4af286274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does the document suggest about currencies?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869534a-e1d4-4c5b-8788-eebeb6213f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_print_docs(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17387b-0b9e-4038-bb77-a27be1f9f127",
   "metadata": {},
   "source": [
    "### Alternative: Compression retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d24d5ac2-2317-45b3-9e2d-809f776bc3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   247.62 MiB, ( 2368.94 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      Metal buffer size =   247.62 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3840.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 2624.94 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2624.95 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   568.02 MiB, ( 3192.95 / 10922.67)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =   568.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   568.00 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Wrap our vectorstore\n",
    "# Use local model\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "model_path = \"/Users/trucvietle/Downloads/llm-models/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "model = LlamaCpp(model_path=model_path,\n",
    "                 n_ctx=8192, n_batch=512,\n",
    "                 n_threads=7, n_gpu_layers=2)\n",
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c63cb73-4b89-40ca-84a0-0f2ed1f97699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8970.05 ms\n",
      "llama_print_timings:      sample time =      11.11 ms /    87 runs   (    0.13 ms per token,  7832.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8969.67 ms /   179 tokens (   50.11 ms per token,    19.96 tokens per second)\n",
      "llama_print_timings:        eval time =  193519.65 ms /    86 runs   ( 2250.23 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  202932.78 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8970.05 ms\n",
      "llama_print_timings:      sample time =      10.86 ms /    77 runs   (    0.14 ms per token,  7093.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10599.29 ms /   459 tokens (   23.09 ms per token,    43.30 tokens per second)\n",
      "llama_print_timings:        eval time =  212124.74 ms /    76 runs   ( 2791.11 ms per token,     0.36 tokens per second)\n",
      "llama_print_timings:       total time =  223182.30 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "> \"Finally, to limit the riskiness of the portfolio, exposure to high yield bonds should be reduced in favour of investment grade corporate bonds.\"\n",
      "This part of the context suggests that the document is discussing the allocation of fixed income assets within a portfolio and provides guidance on reducing exposure to high-yield bonds in favor of investment-grade corporate bonds.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "* Bonds (mentioned multiple times)\n",
      "* Federal Reserve (mentioned)\n",
      "* Interest rates (mentioned)\n",
      "* Inflation (mentioned)\n",
      "* Economy (mentioned)\n",
      "* Credit (mentioned)\n",
      "* Spreads (mentioned)\n",
      "* Duration (mentioned)\n",
      "* Currency (mentioned)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "* Bonds\n",
      "* Bond yields\n",
      "* Monetary policy\n",
      "* Asset allocation\n",
      "* Equities\n",
      "* Fixed income assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8970.05 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    30 runs   (    0.16 ms per token,  6215.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10268.66 ms /   368 tokens (   27.90 ms per token,    35.84 tokens per second)\n",
      "llama_print_timings:        eval time =   75285.93 ms /    29 runs   ( 2596.07 ms per token,     0.39 tokens per second)\n",
      "llama_print_timings:       total time =   85736.55 ms /   397 tokens\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the document suggest about bonds?\"\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207614af-b5ec-4e8c-81f0-9dbefdf9ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "question = \"What does the document suggest about bonds?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40557923-c78e-4fb6-9711-9c0c69688ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on Mistral 7B model (offline)\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"/Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf\"\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_ctx=8192, n_batch=512,\n",
    "            n_threads=7, n_gpu_layers=2,\n",
    "            verbose=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e667cfe-3eda-420a-a4b4-691cdbd7bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "\n",
      "Context:\n",
      "> \"Finally, to limit the riskiness of the portfolio, exposure to high yield bonds should be reduced in favour of investment grade corporate bonds.\"\n",
      "This part of the context suggests that the document is discussing the allocation of fixed income assets within a portfolio and provides guidance on reducing exposure to high-yield bonds in favor of investment-grade corporate bonds.\n",
      "\n",
      "---\n",
      "\n",
      "* Bonds (mentioned multiple times)\n",
      "* Federal Reserve (mentioned)\n",
      "* Interest rates (mentioned)\n",
      "* Inflation (mentioned)\n",
      "* Economy (mentioned)\n",
      "* Credit (mentioned)\n",
      "* Spreads (mentioned)\n",
      "* Duration (mentioned)\n",
      "* Currency (mentioned)\n",
      "\n",
      "---\n",
      "\n",
      "* Bonds\n",
      "* Bond yields\n",
      "* Monetary policy\n",
      "* Asset allocation\n",
      "* Equities\n",
      "* Fixed income assets\n",
      "\n",
      "Question: What does the document suggest about bonds?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the document suggest about bonds?\"\n",
    "\n",
    "contexts = [d.page_content for d in compressed_docs]\n",
    "\n",
    "prompt_start = (\n",
    "    \"Answer the question based on the context below.\\n\\n\"+\n",
    "    \"Context:\\n\"\n",
    ")\n",
    "\n",
    "prompt_end = (\n",
    "    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + \n",
    "    prompt_end\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b73106f8-f19c-4954-962f-39c6aa0795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document suggests that exposure to high-yield bonds should be reduced in favor of investment-grade corporate bonds, indicating that bonds are an important component of the portfolio and their risk is a concern.\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt, echo=True, stream=False, max_tokens=4096)\n",
    "\n",
    "output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308a395-c762-4bd8-8211-6cf764eb572a",
   "metadata": {},
   "source": [
    "## Create a chatbot!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb78560-aa47-4d78-91b1-66aa92c3e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612301d1-aea1-41a7-afc0-9f68aaf04c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# # Invoke the LLM\n",
    "# llm_name = \"gpt-3.5-turbo\"\n",
    "# llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0490753-9ab2-4f15-9ef3-b7ed2735781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    \n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    \n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    \n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    \n",
    "    return qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597e7fc-15fb-4b63-ab12-463778e1a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"../docs/Inview_June_2023.pdfInview_June_2023.pdf\"\n",
    "        self.qa = load_db(self.loaded_file, \"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "            \n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        \n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50cba5-40d1-4dbb-83b2-e54d9891fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot\n",
    "cb = cbfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398ce2e-b679-41ec-b0a0-1451ad4df4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the panels\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd43af3-5fad-427b-a28d-312d67a341ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd36f3-2038-4960-9d98-ba9ea07e9281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
