{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {},
   "source": [
    "# L5: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36329164-8182-42ef-8cce-729b98543cba",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5725255c-8e4e-459b-ab12-29fca393b662",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372ca766-d5c2-409e-9385-1c1b2b775f86",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         DPRContextEncoder, DPRQuestionEncoder\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11034165-7f1b-4a73-b3ca-99ab8f808782",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> To access <code>requirements.txt</code> for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(features):\n",
    "    \n",
    "    norms = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    normalized_features = features / norms\n",
    "    \n",
    "    similarity_matrix = np.inner(normalized_features, normalized_features)\n",
    "    rounded_similarity_matrix = np.round(similarity_matrix, 4)\n",
    "    \n",
    "    return rounded_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86800-66b5-4c28-9bfb-ffe8bc215474",
   "metadata": {},
   "source": [
    "## Pure similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the tallest mountain in the world?\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The highest mountain in the world is Mount Everest.\",\n",
    "    \"Mount Shasta.\",\n",
    "    \"I like my hike in the mountains.\",\n",
    "    \"I am going to a yoga class.\"\n",
    "]\n",
    "\n",
    "question = answers[0]\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23055d3e-3403-44bf-b7bf-997b94c0746a",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.7718, 0.3975, 0.3432, 0.086]\n",
      "Question = What is the tallest mountain in the world?\n",
      "Best answer = What is the tallest mountain in the world?\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "question_embedding = list(model.encode(question))\n",
    "\n",
    "sim = []\n",
    "for answer in answers:\n",
    "    answer_embedding = list(model.encode(answer))\n",
    "    sim.append(cosine_similarity_matrix(np.stack([question_embedding, answer_embedding]))[0,1])\n",
    "\n",
    "print(sim)\n",
    "best_inx = np.argmax(sim)\n",
    "print(f\"Question = {question}\")\n",
    "print(f\"Best answer = {answers[best_inx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42995790-5b01-4105-abeb-670846767af1",
   "metadata": {},
   "source": [
    "## Dual-Encoder inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7373efa19184ed7bb15bb9d89564503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834499e5b77e4ab1a0bdbbf2ffa4649c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dafa4d7bcbe4d39bc60fb72e2d6e39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3474f7fd2f54b68a174b0f201354908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e1972e1abf4a76855de16ffa64d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce21d6a2fcce4da29ef4db7bcbddab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f457d5a5d9410db7a2fe5ad3f3448e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a78a5f3c98c4cd5841dd69c66dcc3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e2de35f162492a9cc015255f3f9003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e1dee5d290455abf577d591abb24c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_tokenizer = AutoTokenizer \\\n",
    "                   .from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "answer_encoder = DPRContextEncoder \\\n",
    "                   .from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "question_tokenizer = AutoTokenizer \\\n",
    "                   .from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "question_encoder = DPRQuestionEncoder \\\n",
    "                   .from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07758200913667679, 0.251725435256958, 0.18663956224918365, 0.22120118141174316, 0.026415381580591202, -0.15785622596740723, 0.32760292291641235, 0.2673285901546478, -0.08503079414367676, 0.1292949914932251] 768\n"
     ]
    }
   ],
   "source": [
    "# Compute the question embeddings\n",
    "question_tokens = question_tokenizer(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "question_embedding = question_encoder(question_tokens).pooler_output.flatten().tolist()\n",
    "\n",
    "print(question_embedding[:10], len(question_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b3c8a2-1556-45cc-8db7-d5339c3e686b",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6253, 0.7346, 0.5965, 0.3168, 0.2565]\n",
      "Question = What is the tallest mountain in the world?\n",
      "Best answer = The highest mountain in the world is Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "for answer in answers:\n",
    "    answer_tokens = answer_tokenizer(answer, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    answer_embedding = answer_encoder(answer_tokens).pooler_output.flatten().tolist() \n",
    "    sim.append(cosine_similarity_matrix(np.stack([question_embedding, answer_embedding]))[0,1])\n",
    "\n",
    "print(sim)\n",
    "best_inx = np.argmax(sim)\n",
    "print(f\"Question = {question}\")\n",
    "print(f\"Best answer = {answers[best_inx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f73-52ff-4676-8ebf-1b607c284830",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
