{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {},
   "source": [
    "# L4: Training a Dual Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf2e4-def8-44c7-a5f2-cf88836e9e22",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5725255c-8e4e-459b-ab12-29fca393b662",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fc6d7-218b-44a3-ac58-6ef41b5faf8f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> To access <code>requirements.txt</code> for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce87a1-5ca5-41ec-b7d2-e44a892f7b9a",
   "metadata": {},
   "source": [
    "## The CrossEntropyLoss 'trick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [4.3, 1.2, 0.05, 1.07],\n",
    "        [0.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 1.03],\n",
    "        [0.23, 0.57, 0.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "print(df.shape)\n",
    "\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916a47b2-ece3-4719-a8bb-162f3a1fd33d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(data):\n",
    "    \n",
    "    target = torch.arange(data.size(0)) # the row?\n",
    "    # print(target)\n",
    "    loss = torch.nn.CrossEntropyLoss()(data, target)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9662, 0.0154, 0.0049, 0.0135],\n",
       "        [0.0165, 0.9541, 0.0150, 0.0145],\n",
       "        [0.0731, 0.0410, 0.7983, 0.0876],\n",
       "        [0.0027, 0.0038, 0.0024, 0.9911]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83bc91c4-fe9f-44d7-8735-c554ec3c4bfd",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3000, 1.2000, 0.0500, 1.0700],\n",
      "        [0.1800, 3.2000, 0.0900, 0.0500],\n",
      "        [0.8500, 0.2700, 2.2000, 1.0300],\n",
      "        [0.2300, 0.5700, 0.1200, 5.1000]])\n",
      "Loss = 0.19657586514949799\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[4.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [0.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 1.0100],\n",
      "        [0.2100, 0.5500, 0.1000, 5.6000]])\n",
      "Loss = 0.12602083384990692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[5.3000, 1.1600, 0.0100, 1.0300],\n",
      "        [0.1400, 4.2000, 0.0500, 0.0100],\n",
      "        [0.8100, 0.2300, 3.2000, 0.9900],\n",
      "        [0.1900, 0.5300, 0.0800, 6.1000]])\n",
      "Loss = 0.07888662070035934\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "\n",
    "for inx in range(3):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5 # increase the diag value\n",
    "    data[non_diag_mask] -= inx*0.02 # decrease the rest of the rows\n",
    "    \n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d4e24-3cb2-416b-8705-9d28e86a6b86",
   "metadata": {},
   "source": [
    "## The Encoder module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3254ee3-4aef-4dca-802f-334275a06d05",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers=3,\n",
    "            norm=torch.nn.LayerNorm([embed_dim]),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        \n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "    \n",
    "    def forward(self, tokenizer_output):\n",
    "        \n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask=tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        \n",
    "        return self.projection(cls_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d2b75-1e6b-4a25-ba17-adcca6f1b11f",
   "metadata": {},
   "source": [
    "![Diagram 1](DLAI-diagram-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7195b4-d7d2-4bf6-850f-a7793afca09b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 812
   },
   "outputs": [],
   "source": [
    "def train_loop(dataset):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # Define the question/answer encoders -- the input embeddings?\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                               output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                             output_embed_size)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                             shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()\n",
    "    ), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    running_loss = []\n",
    "    for _, data_batch in enumerate(dataloader):\n",
    "        # Tokenize the question/answer pairs (each is a batch of 32 questions and 32 answers)\n",
    "        question, answer = data_batch\n",
    "        question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "        answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "\n",
    "        # Compute the embeddings: The output is of dim = 32 x 128\n",
    "        question_embed = question_encoder(question_tok)\n",
    "        answer_embed = answer_encoder(answer_tok)\n",
    "\n",
    "        # Compute similarity scores: A 32x32 matrix\n",
    "        # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "        similarity_scores = question_embed @ answer_embed.T\n",
    "\n",
    "        # We want to maximize the values in the diagonal\n",
    "        target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "        loss = loss_fn(similarity_scores, target)\n",
    "        running_loss += [loss.item()]\n",
    "\n",
    "        # This is where the magic happens\n",
    "        optimizer.zero_grad() # reset optimizer so gradients are all-zero\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f33a1-6615-460c-af10-17aaa63b7797",
   "metadata": {},
   "source": [
    "## Training in multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff76b1a9-02f3-4090-a1cb-e494cbe7ea8f",
   "metadata": {
    "height": 931
   },
   "outputs": [],
   "source": [
    "def train(dataset, num_epochs=10):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "    \n",
    "    # Define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # Define the dataloader, optimizer and loss function    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for idx, data_batch in enumerate(dataloader):\n",
    "            # Tokenize the question/answer pairs (each is a batc of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)\n",
    "            \n",
    "            # Compute the embeddings: The output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "    \n",
    "            # Compute similarity scores: A 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "    \n",
    "            # We want to maximize the values in the diagonal\n",
    "            target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if idx == n_iters-1:\n",
    "                print(f\"Epoch {epoch}, loss = \", np.mean(running_loss))\n",
    "    \n",
    "            # This is where the magic happens\n",
    "            optimizer.zero_grad() # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ce22f-b921-49af-b932-16e9926f945c",
   "metadata": {},
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9733e9a-2704-4bb1-b7ff-0d6c710d688a",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who played bubba in the tv series in the heat ...</td>\n",
       "      <td>Carlos Alan Autry Jr. (also known for a period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where did the 2017 tour de france start</td>\n",
       "      <td>The 3,540 km (2,200 mi)-long race commenced wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who is the chess champion of the world</td>\n",
       "      <td>Current world champion Magnus Carlsen won the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who scored the most hat tricks in football</td>\n",
       "      <td>Cristiano Ronaldo and Messi have scored three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what do you need to be an ontario scholar</td>\n",
       "      <td>Ontario Scholars are high school graduates in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  who played bubba in the tv series in the heat ...   \n",
       "1            where did the 2017 tour de france start   \n",
       "2             who is the chess champion of the world   \n",
       "3         who scored the most hat tricks in football   \n",
       "4          what do you need to be an ontario scholar   \n",
       "\n",
       "                                             answers  \n",
       "0  Carlos Alan Autry Jr. (also known for a period...  \n",
       "1  The 3,540 km (2,200 mi)-long race commenced wi...  \n",
       "2  Current world champion Magnus Carlsen won the ...  \n",
       "3  Cristiano Ronaldo and Messi have scored three ...  \n",
       "4  Ontario Scholars are high school graduates in ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep=\"\\t\", nrows=300)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['questions'], self.data.iloc[idx]['answers']\n",
    "\n",
    "dataset = MyDataset('../data/nq_sample.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb3d80-8635-4800-83c2-ca64991b98dd",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(num_epochs = 5)</code>:</b> The <code>num_epochs</code> is set to <code>5</code> for speedier execution. You may train the model \n",
    "using a higher number of epochs by changing this parameter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eca1874-c660-4336-b411-5ab470b0a9b5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09a17a72b8412db90dfeef59ab5c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss =  3.6094016075134276\n",
      "Epoch 1, loss =  3.50247483253479\n",
      "Epoch 2, loss =  3.399679350852966\n",
      "Epoch 3, loss =  3.354437756538391\n",
      "Epoch 4, loss =  3.30797073841095\n"
     ]
    }
   ],
   "source": [
    "qe, ae = train(dataset, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29c17cfc-02ad-4a02-99fc-da55da8c17ec",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([-0.2737, -0.2208, -0.5643, -0.1300, -0.1593], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the tallest mountain in the world?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = qe(question_tok)[0]\n",
    "\n",
    "print(question_tok)\n",
    "print(question_emb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a33357-6507-4ae9-a638-2de4af933c86",
   "metadata": {
    "height": 302
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 2054, 2003, 1996, 3284, 3137, 1999, 1996, 2088, 1029,  102]]), tensor([[  101,  1996, 13747,  3137,  1999,  1996,  2088,  2003,  4057, 23914,\n",
      "          1012,   102]]), tensor([[ 101, 2040, 2003, 6221, 9457, 1029,  102]])]\n",
      "tensor([-0.5291, -0.4120, -0.6487, -0.2240,  0.3556], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3455, -0.1610, -0.7036, -0.2872,  0.5947], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.6597, -0.0751, -0.5807, -0.1091,  0.1851], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    \"What is the highest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5a9a16-016c-4240-9a3f-847c5a7d5624",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4778, 2.3825, 3.1635], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc3b8-b5cc-40ec-83a0-d4c56fd481b4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45adda-f136-43c4-90a3-4393b45f75ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
