{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad3a7d-5d67-4772-a042-ec761e83d9a3",
   "metadata": {},
   "source": [
    "# Lesson 2 - Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbf1cc-52b7-4793-aa51-6257850334ed",
   "metadata": {},
   "source": [
    "### Import the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc3632c-12bc-4de7-a36e-2e3e3796274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm.auto import tqdm\n",
    "from DLAIUtils import Utils\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160db7c1-73e9-4df6-93fc-cbebb6034b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key\n",
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169625e-67d8-48aa-aa25-8cf30e3beda1",
   "metadata": {},
   "source": [
    "### Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c3f4e9-c618-4c13-bba5-f347357dd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create a Pinecone index (vector DB)\n",
    "utils = Utils()\n",
    "INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "  pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',\n",
    "  spec=ServerlessSpec(cloud='aws', region='us-west-2'))\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73a121-2eb8-449f-a0f3-cde19a6292c8",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd7421-e286-40b6-a299-91ba9432788c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>(Note: <code>max_articles_num = 500</code>):</b> To achieve a more comprehensive context for the Language Learning Model, a larger number of articles is generally more beneficial. In this lab, we've initially set <code>max_articles_num</code> to 500 for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, consider increasing this value to 750 or 1,000. You'll likely notice that the context provided to the LLM becomes richer and better. You can experiment by gradually raising this variable for different queries to observe the improvements in the LLM's contextual understanding.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dc461c-1d76-4bf2-915a-4d1eba3a613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-0</td>\n",
       "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-1</td>\n",
       "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-2</td>\n",
       "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-3</td>\n",
       "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-4</td>\n",
       "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           metadata  \\\n",
       "1  1-0  {'chunk': 0, 'source': 'https://simple.wikiped...   \n",
       "2  1-1  {'chunk': 1, 'source': 'https://simple.wikiped...   \n",
       "3  1-2  {'chunk': 2, 'source': 'https://simple.wikiped...   \n",
       "4  1-3  {'chunk': 3, 'source': 'https://simple.wikiped...   \n",
       "5  1-4  {'chunk': 4, 'source': 'https://simple.wikiped...   \n",
       "\n",
       "                                              values  \n",
       "1  [-0.011254455894231796, -0.01698738895356655, ...  \n",
       "2  [-0.0015197008615359664, -0.007858820259571075...  \n",
       "3  [-0.009930099360644817, -0.012211072258651257,...  \n",
       "4  [-0.011600767262279987, -0.012608098797500134,...  \n",
       "5  [-0.026462381705641747, -0.016362832859158516,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_articles_num = 500\n",
    "# This corpus has already been embedded\n",
    "df = pd.read_csv('./data/wiki.csv', nrows=max_articles_num)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd62d36-db99-42b9-9a21-b005f292a26a",
   "metadata": {},
   "source": [
    "### Prepare the Embeddings and Upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd08b86-ba97-4f7d-9159-c5a54acc4657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88312fe412d94348b4dcadff20190ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prepped = []\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    meta = ast.literal_eval(row['metadata'])\n",
    "    prepped.append({'id':row['id'], \n",
    "                    'values':ast.literal_eval(row['values']), \n",
    "                    'metadata':meta})\n",
    "    if len(prepped) >= 250:\n",
    "        index.upsert(prepped)\n",
    "        prepped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3f65fb-161a-455e-ae72-42ef4dae6d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 500}},\n",
       " 'total_vector_count': 500}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()\n",
    "# Check that the dimension is same as that of text-embedding-ada-002 (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913af28d-88a0-4e0c-a29d-b0e53f7f4bb2",
   "metadata": {},
   "source": [
    "### Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61a3407-31b0-4ee7-8f2c-28f3648586e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = utils.get_openai_api_key()\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_embeddings(articles, model=\"text-embedding-ada-002\"):\n",
    "   return openai_client.embeddings.create(input = articles, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ecb5a-664b-4eb2-bc81-7ee18225e8aa",
   "metadata": {},
   "source": [
    "### Run Your Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e21b02-7cd4-4c56-b4de-60da0b656722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancient history \n",
      "\n",
      "Before the middle of the sixth century BCE, Afghanistan was held by the Medes.\n",
      "Then the Achaemenids took over control of the land and made it part of the Persian empire. Alexander the great defeated and conquered the Persian Empire in 330 BCE. He founded some cities in the area. The people used Macedonian culture and language. After Alexander, Greco-Bactrians, Scythians, Kushans, Parthians and Sassanians ruled the area.\n",
      "\n",
      "Kushans spread Buddhism from India in the 1st century BCE, and Buddhism remained an important religion in the area until the Islamic conquest in the 7th century CE.\n",
      "\n",
      "The Buddhas of Bamiyan were giant statues, a reminder of Buddhism in Afghanistan. They were destroyed by the Taliban in 2001. There were international protests. The Taliban believe that the ancient statues were un-Islamic and that they had a right to destroy them.\n",
      "\n",
      "Medieval history \n",
      "\n",
      "Arabs introduced Islam in the 7th century and slowly began spreading the new religion. In the 9th and 10th centuries, many local Islamic dynasties rose to power inside Afghanistan. One of the earliest was the Tahirids, whose kingdom included Balkh and Herat; they established independence from the Abbasids in 820. The Tahirids were succeeded in about 867 by the Saffarids of Zaranj in western Afghanistan. Local princes in the north soon became feudatories of the powerful Samanids, who ruled from Bukhara. From 872 to 999, north of the Hindu Kush in Afghanistan enjoyed a golden age under Samanid rule.\n",
      "History \n",
      "Early forms of algebra were developed by the Babylonians and the Greek geometers such as Hero of Alexandria. However the word \"algebra\" is a Latin form of the Arabic word Al-Jabr (\"casting\") and comes from a mathematics book Al-Maqala fi Hisab-al Jabr wa-al-Muqabilah, (\"Essay on the Computation of Casting and Equation\") written in the 9th century by a Persian mathematician, Muhammad ibn Mūsā al-Khwārizmī, who was a Muslim born in Khwarizm in Uzbekistan. He flourished under Al-Ma'moun in Baghdad, Iraq through 813-833 AD, and died around 840 AD. The book was brought into Europe and translated into Latin in the 12th century. The book was then given the name 'Algebra'. (The ending of the mathematician's name, al-Khwarizmi, was changed into a word easier to say in Latin, and became the English word algorithm).\n",
      "\n",
      "Examples\n",
      "\n",
      "Here is a simple example of an algebra problem:\n",
      "\n",
      "Sue has 12 candies, and Ann has 24 candies. They decide to share so that they have the same number of candies.  How many candies will each have?\n",
      "\n",
      "These are the steps you can use to solve the problem:\n",
      "In the 10th century, the local Ghaznavids turned Ghazni into their capital and firmly established Islam throughout all areas of Afghanistan, except the Kafiristan region in the northeast. Mahmud of Ghazni, a great Ghaznavid sultan, conquered the Multan and Punjab region, and carried raids into the heart of India. Mohammed bin Abdul Jabbar Utbi (Al-Utbi), a historian from the 10th century, wrote that thousands of \"Afghans\" were in the Ghaznavid army. The Ghaznavid dynasty was replaced by the Ghorids of Ghor in the late 12th century, who reconquered Ghaznavid territory in the name of Islam and ruled it until 1206. The Ghorid army also included ethnic Afghans.\n",
      "\n",
      "Afghanistan was recognized as Khorasan, meaning \"land of the rising sun,\" which was a prosperous and independent geographic region reaching as far as the Indus River.\n",
      "\n",
      "All the major cities of modern Afghanistan were centers of science and culture in the past. The New Persian literature arose and flourished in the area. The early Persian poets such as Rudaki were from what is now Afghanistan. Moreover, Ferdowsi, the author of Shahnameh, the national epic of Iran, and Rumi, the famous Sufi poet, were also from modern-day Afghanistan. It has produced scientists such as Avicenna, Al-Farabi, Al-Biruni, Omar Khayyám, Al-Khwarizmi, and many others who are widely known for their important contributions in areas such as mathematics, astronomy, medicine, physics, geography, and geology. It remained the cultural capital of Persia until the devastating Mongol invasion in the 13th century.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Babylon gate?\"\n",
    "\n",
    "embed = get_embeddings([query])\n",
    "res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
    "text = [r['metadata']['text'] for r in res['matches']]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f6542-5c1d-460f-8e39-1aea8b222909",
   "metadata": {},
   "source": [
    "### Build the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e12c511-97b6-4c11-a9eb-1fff15834cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "\n",
      "Context:\n",
      "Now, 150 years later, it really is a big city.\n",
      "\n",
      "In modern times many cities have grown bigger and bigger. The whole area is often called a  \"metropolis\"  and can sometimes include several small ancient towns and villages. The metropolis of London includes London, Westminster, and many old villages such as Notting Hill, Southwark, Richmond, Greenwich, etc. The part that is officially known as the \" City of London \" only takes up one square mile. The rest is known as \"Greater London. \" Many other cities have grown in the same way.\n",
      "\n",
      "These giant cities can be exciting places to live, and many people can find good jobs there, but modern cities also have many problems. Many people cannot find jobs in the cities and have to get money by begging or by crime. Automobiles, factories, and waste create a lot of pollution that makes people sick.\n",
      "\n",
      "Urban history \n",
      "\n",
      "Urban history is history of civilization. The first cities were made in ancient times, as soon as people began to create civilization . Famous ancient cities which fell to ruins included Babylon, Troy, Mycenae and Mohenjo-daro.\n",
      "\n",
      "Benares in northern India is one among the ancient cities which has a history of more than 3000 years. Other cities that have existed since ancient times are Athens in Greece, Rome and Volterra in Italy, Alexandria in Egypt and York in England.\n",
      "\n",
      "In Europe, in the Middle Ages, being a city was a special privilege, granted by nobility. Cities that fall into this category, usually had (or still have) city walls. The people who lived in the city were privileged over those who did not. Medieval cities that still have walls include Carcassonne in France, Tehran in Iran, Toledo in Spain and Canterbury in England.\n",
      "\n",
      "Features\n",
      "\n",
      "Infrastructure\n",
      "\n",
      "---\n",
      "\n",
      "Ancient history \n",
      "\n",
      "Before the middle of the sixth century BCE, Afghanistan was held by the Medes.\n",
      "Then the Achaemenids took over control of the land and made it part of the Persian empire. Alexander the great defeated and conquered the Persian Empire in 330 BCE. He founded some cities in the area. The people used Macedonian culture and language. After Alexander, Greco-Bactrians, Scythians, Kushans, Parthians and Sassanians ruled the area.\n",
      "\n",
      "Kushans spread Buddhism from India in the 1st century BCE, and Buddhism remained an important religion in the area until the Islamic conquest in the 7th century CE.\n",
      "\n",
      "The Buddhas of Bamiyan were giant statues, a reminder of Buddhism in Afghanistan. They were destroyed by the Taliban in 2001. There were international protests. The Taliban believe that the ancient statues were un-Islamic and that they had a right to destroy them.\n",
      "\n",
      "Medieval history \n",
      "\n",
      "Arabs introduced Islam in the 7th century and slowly began spreading the new religion. In the 9th and 10th centuries, many local Islamic dynasties rose to power inside Afghanistan. One of the earliest was the Tahirids, whose kingdom included Balkh and Herat; they established independence from the Abbasids in 820. The Tahirids were succeeded in about 867 by the Saffarids of Zaranj in western Afghanistan. Local princes in the north soon became feudatories of the powerful Samanids, who ruled from Bukhara. From 872 to 999, north of the Hindu Kush in Afghanistan enjoyed a golden age under Samanid rule.\n",
      "\n",
      "---\n",
      "\n",
      "Prehistory \n",
      "\n",
      "Archaeologists digging a cave in what is now northeastern Afghanistan (in Badakhshan), discovered that people lived in the country as early as 100,000 years ago. They found the skull of a Neanderthal, or early human, as well as tools from about 30,000 years ago. In other parts of Afghanistan, archaeologists uncovered pottery and tools that are 4,000 to 11,000 years old—evidence that Afghans were among the first people in the world to grow crops and raise animals.\n",
      "\n",
      "Farmers and herders settled in the plains surrounding the Hindu Kush as early as 7000 B.C. These people may have grown rich off the lapis lazuli they found along riverbeds, which they traded to early city sites to the west, across the Iranian plateau and Mesopotamia. As farms and villages grew and thrived in Afghanistan, these ancient people eventually invented irrigation (digging ditches for water so it flows to crops) that allowed them to grow crops on the northern Afghanistan desert plains. This civilization (advanced state of organization) is today called BMAC (Bactria–Margiana Archaeological Complex), or the \"Oxus civilization\".\n",
      "\n",
      "The Oxus civilization expanded as far east as western edge of the Indus Valley during the period between 2200 and 1800 B.C. These people, who were the ancestors of the Indo-Aryans, used the term \"Aryan\" to identify their ethnicity, culture, and religion. Scholars know this when they read the ancient texts of these people; the Avesta of Iranic peoples and the Vedas of Indo-Aryans.\n",
      "\n",
      "Zoroaster, the founder of the Zoroastrian religion, the world's earliest monotheistic religion, (meaning a religion believing in one god) lived in the area (somewhere north of today's Afghanistan), around 1000 B.C.\n",
      "\n",
      "Question: write an article titled: what is the Babylon gate?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "query = \"write an article titled: what is the Babylon gate?\"\n",
    "embed = get_embeddings([query])\n",
    "res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
    "\n",
    "contexts = [\n",
    "    x['metadata']['text'] for x in res['matches']\n",
    "]\n",
    "\n",
    "prompt_start = (\n",
    "    \"Answer the question based on the context below.\\n\\n\"+\n",
    "    \"Context:\\n\"\n",
    ")\n",
    "\n",
    "prompt_end = (\n",
    "    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + \n",
    "    prompt_end\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256f222-dad4-4ad1-bcab-a68bd24687bf",
   "metadata": {},
   "source": [
    "### Get the Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d42034-620c-42bd-b91a-01e1573cbb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "The Babylon Gate, also known as the Ishtar Gate, was a monumental entrance to the ancient city of Babylon, located in present-day Iraq. It was built during the reign of King Nebuchadnezzar II in the 6th century BCE and was considered one of the most impressive architectural achievements of the time.\n",
      "\n",
      "The gate was part of a larger project to rebuild the city of Babylon, which had been destroyed by the Assyrians. It was constructed using glazed bricks, with intricate designs and images of animals and gods adorning its walls. The gate was dedicated to the goddess Ishtar, the patron deity of Babylon, and was meant to impress and intimidate visitors with its grandeur and power.\n",
      "\n",
      "The Babylon Gate was not only a symbol of the city's wealth and strength, but it also served as a defensive structure. The gate was part of a larger wall that surrounded the city, and its imposing size and design were meant to deter any potential invaders.\n",
      "\n",
      "The gate was also an important religious site, as it was believed to be the place where the gods entered and exited the city. It was a popular destination for pilgrims and was often used for religious ceremonies and processions.\n",
      "\n",
      "Unfortunately, the Babylon Gate, along with the rest of the city, fell into ruin over time. It was eventually excavated by German archaeologist Robert Koldewey in the early 20th century, and many of its original bricks and decorations were discovered and reconstructed.\n",
      "\n",
      "Today, the Babylon Gate is considered one of the most significant archaeological finds in the world. Its intricate design and historical significance have made it a popular tourist attraction, drawing visitors from all over the world to marvel at its beauty and learn about the ancient city of Babylon. \n"
     ]
    }
   ],
   "source": [
    "res = openai_client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=636,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print('-' * 80)\n",
    "print(res.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d19ff5-f61a-41bd-b3f4-3626ccea65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   341.33 MiB, (  341.39 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "llm_load_tensors:      Metal buffer size =   341.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, (  406.95 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  406.97 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   556.02 MiB, (  962.97 / 10922.67)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =   556.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   556.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Try on Mistral 7B model (offline)\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"/Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf\"\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_ctx=8192, n_batch=512,\n",
    "            n_threads=7, n_gpu_layers=2,\n",
    "            verbose=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d56fd87-fb95-4364-95c3-b0025d4021c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m llm(prompt, echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama.py:1391\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1329\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1392\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1393\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1394\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1395\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1396\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1397\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1398\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1399\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1400\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1401\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1402\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1403\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1404\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1405\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1406\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1407\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1408\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1409\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1410\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1411\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1412\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1413\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1414\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1415\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1416\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1417\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama.py:1324\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1324\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama.py:850\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    848\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 850\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    851\u001b[0m     prompt_tokens,\n\u001b[1;32m    852\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    853\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    854\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m    855\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m    856\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    857\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m    858\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m    859\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m    860\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m    861\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    862\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m    863\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    864\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m    865\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m    866\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m    869\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama.py:624\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[1;32m    625\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    626\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    627\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m         penalize_nl\u001b[38;5;241m=\u001b[39mpenalize_nl,\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    644\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama.py:437\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    433\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    435\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/_internals.py:309\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    310\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    311\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    312\u001b[0m )\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/llama_cpp.py:1593\u001b[0m, in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = llm(prompt, echo=True, stream=False, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9771b17-2a91-4a67-8c22-c6b03b0f160f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
