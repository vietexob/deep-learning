{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad3a7d-5d67-4772-a042-ec761e83d9a3",
   "metadata": {},
   "source": [
    "# Lesson 2 - Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbf1cc-52b7-4793-aa51-6257850334ed",
   "metadata": {},
   "source": [
    "### Import the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc3632c-12bc-4de7-a36e-2e3e3796274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm.auto import tqdm\n",
    "from DLAIUtils import Utils\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160db7c1-73e9-4df6-93fc-cbebb6034b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key\n",
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169625e-67d8-48aa-aa25-8cf30e3beda1",
   "metadata": {},
   "source": [
    "### Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c3f4e9-c618-4c13-bba5-f347357dd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# # Create a Pinecone index (vector DB)\n",
    "# utils = Utils()\n",
    "# INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
    "# if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "#   pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',\n",
    "#   spec=ServerlessSpec(cloud='aws', region='us-west-2'))\n",
    "\n",
    "# index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73a121-2eb8-449f-a0f3-cde19a6292c8",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd7421-e286-40b6-a299-91ba9432788c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>(Note: <code>max_articles_num = 500</code>):</b> To achieve a more comprehensive context for the Language Learning Model, a larger number of articles is generally more beneficial. In this lab, we've initially set <code>max_articles_num</code> to 500 for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, consider increasing this value to 750 or 1,000. You'll likely notice that the context provided to the LLM becomes richer and better. You can experiment by gradually raising this variable for different queries to observe the improvements in the LLM's contextual understanding.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dc461c-1d76-4bf2-915a-4d1eba3a613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-0</td>\n",
       "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-1</td>\n",
       "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-2</td>\n",
       "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-3</td>\n",
       "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-4</td>\n",
       "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
       "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           metadata  \\\n",
       "1  1-0  {'chunk': 0, 'source': 'https://simple.wikiped...   \n",
       "2  1-1  {'chunk': 1, 'source': 'https://simple.wikiped...   \n",
       "3  1-2  {'chunk': 2, 'source': 'https://simple.wikiped...   \n",
       "4  1-3  {'chunk': 3, 'source': 'https://simple.wikiped...   \n",
       "5  1-4  {'chunk': 4, 'source': 'https://simple.wikiped...   \n",
       "\n",
       "                                              values  \n",
       "1  [-0.011254455894231796, -0.01698738895356655, ...  \n",
       "2  [-0.0015197008615359664, -0.007858820259571075...  \n",
       "3  [-0.009930099360644817, -0.012211072258651257,...  \n",
       "4  [-0.011600767262279987, -0.012608098797500134,...  \n",
       "5  [-0.026462381705641747, -0.016362832859158516,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_articles_num = 500\n",
    "# This corpus has already been embedded\n",
    "df = pd.read_csv('./data/wiki.csv', nrows=max_articles_num)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd62d36-db99-42b9-9a21-b005f292a26a",
   "metadata": {},
   "source": [
    "### Prepare the Embeddings and Upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd08b86-ba97-4f7d-9159-c5a54acc4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepped = []\n",
    "# for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "#     meta = ast.literal_eval(row['metadata'])\n",
    "#     prepped.append({'id':row['id'], \n",
    "#                     'values':ast.literal_eval(row['values']), \n",
    "#                     'metadata':meta})\n",
    "#     if len(prepped) >= 250:\n",
    "#         index.upsert(prepped)\n",
    "#         prepped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3f65fb-161a-455e-ae72-42ef4dae6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.describe_index_stats()\n",
    "# # Check that the dimension is same as that of text-embedding-ada-002 (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac6bb9-6333-461d-8e71-10626959876a",
   "metadata": {},
   "source": [
    "### Store in local vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98111070-cf6f-4aa5-80fb-852c0d4df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in the local vector DB\n",
    "import chromadb\n",
    "\n",
    "persist_directory = '../docs/chroma-pinecone/'\n",
    "!rm -rf ../docs/chroma-pinecone/  # remove old database files if any\n",
    "\n",
    "client = chromadb.PersistentClient(path=persist_directory)\n",
    "collection = client.get_or_create_collection(name=\"dl-ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c3acd6-56f3-4140-aa89-be9c6a6f9dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf3a874da0749e4ba8855518d288d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    meta = ast.literal_eval(row['metadata'])\n",
    "    ids.append(row[\"id\"])\n",
    "    embeddings.append(ast.literal_eval(row[\"values\"]))\n",
    "    metadatas.append(meta)\n",
    "    documents.append(meta[\"text\"])\n",
    "    \n",
    "collection.add(embeddings=embeddings, metadatas=metadatas,\n",
    "               ids=ids, documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112b691b-b710-4637-9373-fbc4bb42e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913af28d-88a0-4e0c-a29d-b0e53f7f4bb2",
   "metadata": {},
   "source": [
    "### Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61a3407-31b0-4ee7-8f2c-28f3648586e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = utils.get_openai_api_key()\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_embeddings(articles, model=\"text-embedding-ada-002\"):\n",
    "   return openai_client.embeddings.create(input = articles, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "980bbc09-8173-4173-8a50-cb166d0c2735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 in the collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "vectordb = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"dl-ai\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "print(\"There are\", vectordb._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ecb5a-664b-4eb2-bc81-7ee18225e8aa",
   "metadata": {},
   "source": [
    "### Run Your Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e21b02-7cd4-4c56-b4de-60da0b656722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the Babylon gate?\"\n",
    "\n",
    "# embed = get_embeddings([query])\n",
    "# res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
    "# text = [r['metadata']['text'] for r in res['matches']]\n",
    "# print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eaa2239-5290-4efb-a950-c1573c4a0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + \n",
    "                                   d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "044b4b31-018d-42ca-b4b2-80803cd17553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Ezekiel 28:13-14\n",
      "13. Thou hast been in Eden the garden of God; every precious stone was thy covering, the sardius, topaz, and the diamond, the beryl, the onyx, and the jasper, the sapphire, the emerald, and the carbuncle and gold: the workmanship of thy tabrets and of thy pipes was prepared in thee in the day that thou wast created.\n",
      "14. Thou art the anointed cherub that covereth; and I have set thee so: thou wast upon the holy mountain of God; thou hast walked up and down in the midst of the stones of fire.\n",
      "\n",
      "It describes the sound of their wings, \"like the roar of rushing waters.\"\n",
      "\n",
      "Ezekiel 10:5-7 ; Ezekiel 10:8 reveals that they have hands like a man under their wings .\n",
      "\n",
      "Ezekiel 1:7 KJV reveals that they look like man but are different because they have \"straight feet\" and four wings and four faces.\n",
      "\n",
      "Ezekiel ch 1, and 10 describe the cherubim creatures ascending and descending from the earth with wheels. Ezekiel 1:14-20 ; Ezekiel 10:16\n",
      "\n",
      "Ezekiel 10:9-13 describes what the wheels appeared to look like, and how they moved around, how they moved or flew through the sky quickly but turned not as they went; and how the inside workings of the wheels appeared to be \"a wheel in the midst of a wheel\" and that the color of the wheels was the color of \"Amber\" Stone. There are four separate wheels in both accounts, one for each single cherub which is there.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Ancient history \n",
      "\n",
      "Before the middle of the sixth century BCE, Afghanistan was held by the Medes.\n",
      "Then the Achaemenids took over control of the land and made it part of the Persian empire. Alexander the great defeated and conquered the Persian Empire in 330 BCE. He founded some cities in the area. The people used Macedonian culture and language. After Alexander, Greco-Bactrians, Scythians, Kushans, Parthians and Sassanians ruled the area.\n",
      "\n",
      "Kushans spread Buddhism from India in the 1st century BCE, and Buddhism remained an important religion in the area until the Islamic conquest in the 7th century CE.\n",
      "\n",
      "The Buddhas of Bamiyan were giant statues, a reminder of Buddhism in Afghanistan. They were destroyed by the Taliban in 2001. There were international protests. The Taliban believe that the ancient statues were un-Islamic and that they had a right to destroy them.\n",
      "\n",
      "Medieval history \n",
      "\n",
      "Arabs introduced Islam in the 7th century and slowly began spreading the new religion. In the 9th and 10th centuries, many local Islamic dynasties rose to power inside Afghanistan. One of the earliest was the Tahirids, whose kingdom included Balkh and Herat; they established independence from the Abbasids in 820. The Tahirids were succeeded in about 867 by the Saffarids of Zaranj in western Afghanistan. Local princes in the north soon became feudatories of the powerful Samanids, who ruled from Bukhara. From 872 to 999, north of the Hindu Kush in Afghanistan enjoyed a golden age under Samanid rule.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Berlin (; ) is the capital city of Germany. It is the largest city in the European Union by population, with around 3.7 million people in 2020. Berlin residents come from about 190 different countries.\n",
      "\n",
      "The city is in the eastern part of Germany in Central Europe and is surrounded by many forests and lakes. Berlin has an area of . The rivers Havel, Dahme and Spree run through Berlin. It has a temperate climate.\n",
      "\n",
      "Berlin is home to many famous buildings and monuments, like the Siegessäule, the Brandenburger Tor, the Reichstag and the boulevard Unter den Linden. On the boulevard is the Humboldt University. The city has many nightlife options.\n",
      "\n",
      "Berlin is an important city for the history of Germany. The King of Prussia and the Emperor of Germany lived in Berlin. The government of Germany was in Berlin for many years. Bombs destroyed many buildings in the city in World War Two. The city was split into West Berlin and East Berlin after World War Two. After the Berlin Wall was built in 1961 very few people were allowed to cross from East Berlin into West Berlin. The wall divided the city until 1989 when the East German government decided to allow anyone to cross, and people decided to tear down the wall.\n",
      "\n",
      "Berlin is a world city of culture, start ups, politics, media and science. There are a lot of technology companies in the city. They are important for the city's economy. Many planes and trains travel to and from Berlin because the city is an important place for tourism and business.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the Babylon gate?\"\n",
    "\n",
    "res = vectordb.similarity_search(question, k=3)\n",
    "# text = [r.page_content for r in res]\n",
    "# print('\\n'.join(text))\n",
    "pretty_print_docs(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace72623-737a-46b3-8256-d10cb54beed0",
   "metadata": {},
   "source": [
    "### Try to compress the contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a522ba-bde5-4de3-b883-03cf31a569d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Invoke the LLM\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "\n",
    "# Wrap our vectorstore\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e0ec6a-e402-4211-904e-e6e0f5147077",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the Berlin wall?\"\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09339ee9-aae6-49f5-99c8-4c58cce2929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Berlin Wall\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "The city was split into West Berlin and East Berlin after World War Two. After the Berlin Wall was built in 1961 very few people were allowed to cross from East Berlin into West Berlin. The wall divided the city until 1989 when the East German government decided to allow anyone to cross, and people decided to tear down the wall.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "The Berlin Wall was built by the communist government of East Germany between the two halves of Berlin.\n"
     ]
    }
   ],
   "source": [
    "# compressed_docs\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f6542-5c1d-460f-8e39-1aea8b222909",
   "metadata": {},
   "source": [
    "### Build the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e12c511-97b6-4c11-a9eb-1fff15834cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "\n",
      "Context:\n",
      "Berlin Wall\n",
      "\n",
      "---\n",
      "\n",
      "The city was split into West Berlin and East Berlin after World War Two. After the Berlin Wall was built in 1961 very few people were allowed to cross from East Berlin into West Berlin. The wall divided the city until 1989 when the East German government decided to allow anyone to cross, and people decided to tear down the wall.\n",
      "\n",
      "---\n",
      "\n",
      "The Berlin Wall was built by the communist government of East Germany between the two halves of Berlin.\n",
      "\n",
      "Question: Write a short article titled: What is the Berlin wall?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# query = \"write an article titled: what is the Babylon gate?\"\n",
    "query = \"Write a short article titled: What is the Berlin wall?\"\n",
    "\n",
    "# embed = get_embeddings([query])\n",
    "# res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
    "\n",
    "# contexts = [\n",
    "#     x['metadata']['text'] for x in res['matches']\n",
    "# ]\n",
    "\n",
    "contexts = [d.page_content for d in compressed_docs]\n",
    "\n",
    "prompt_start = (\n",
    "    \"Answer the question based on the context below.\\n\\n\"+\n",
    "    \"Context:\\n\"\n",
    ")\n",
    "\n",
    "prompt_end = (\n",
    "    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) + \n",
    "    prompt_end\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256f222-dad4-4ad1-bcab-a68bd24687bf",
   "metadata": {},
   "source": [
    "### Get the Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8d42034-620c-42bd-b91a-01e1573cbb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      " The Berlin Wall was a physical barrier that divided the city of Berlin into two halves, East Berlin and West Berlin, after World War Two. It was built in 1961 by the communist government of East Germany, in an effort to prevent people from fleeing to the democratic West. The wall was heavily guarded and very few people were allowed to cross from East Berlin into West Berlin. This division lasted for almost three decades until 1989, when the East German government made the historic decision to allow anyone to cross the wall. This led to a momentous event in history, as people from both sides came together to tear down the wall, symbolizing the end of the Cold War and the reunification of Germany. The Berlin Wall serves as a reminder of the political and social tensions that existed during the Cold War era, and its fall represents the triumph of freedom and unity over division and oppression. Today, fragments of the wall can still be found in various locations, serving as a powerful symbol of hope and resilience.\n"
     ]
    }
   ],
   "source": [
    "res = openai_client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=636,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print('-' * 80)\n",
    "print(res.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d19ff5-f61a-41bd-b3f4-3626ccea65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   341.33 MiB, (  341.39 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "llm_load_tensors:      Metal buffer size =   341.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/trucvietle/miniforge3/envs/llm-env/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, (  406.95 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  406.97 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   556.02 MiB, (  962.97 / 10922.67)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =   556.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   556.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Try on Mistral 7B model (offline)\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"/Users/trucvietle/Downloads/llm-models/mistral-7b-instruct-v0.1.Q6_K.gguf\"\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_ctx=8192, n_batch=512,\n",
    "            n_threads=7, n_gpu_layers=2,\n",
    "            verbose=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff7b0119-b256-400d-963d-f55ac6ffb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"What is the Babylon gate?\"\n",
    "# output = llm(prompt, echo=True, stream=False, max_tokens=4096)\n",
    "\n",
    "# output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "# print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7383f51c-6509-4598-97fd-5598f7604005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Berlin Wall was a physical barrier that separated the city of Berlin into two halves, West Berlin and East Berlin after the end of World War Two. It was constructed by the communist government of East Germany in 1961 and remained standing for almost 30 years until it was finally torn down in 1989. For most of its existence very few people were allowed to cross from East Berlin into West Berlin, making it a symbol of division and separation. However, when the East German government decided to allow anyone to cross in 1989, thousands of people flocked to the wall and began tearing it down with their own hands. Today, the Berlin Wall stands as a reminder of the Cold War era and a testament to the power of human freedom.\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt, echo=True, stream=False, max_tokens=4096)\n",
    "\n",
    "output_str = output[\"choices\"][0][\"text\"].replace(prompt, \"\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9bfd2-3357-4c27-9c7c-b295cec6ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
